```{r}
library(tidyverse)
library(dplyr)
library(rstatix)
library(qualtRics)
library(lme4)
library(jsonlite)
library(lubridate)
```

```{r}
cloudresearch <- fetch_survey(surveyID = "SV_8ccaK54JnClG1Bs",
                              label = FALSE,
                              convert = FALSE) %>% 
  filter(attn_check == "4" &
         attn_check_4_TEXT == "8")

time_limit <- cloudresearch %>% 
  filter(`Duration (in seconds)`< 1800)

# Count the number of participants in each condition
time_limit %>% 
  group_by(Condition) %>% 
  count(ResponseId) %>% 
  mutate(sum = sum(n)) %>% 
  distinct(Condition, sum)

```

```{r}
# Memory check
cloudresearch %>% 
  select(Condition, memory_check) %>% 
  mutate(memory_check = case_when(memory_check == 1 ~ "high-agency",
                                  memory_check == 2 ~ "low-agency",
                                  memory_check == 3 ~ "unstructured",
                                  memory_check == 4 ~ "control")) %>% 
  count(Condition == memory_check)


# PIVOT
cleaned_names <- time_limit %>% 
  rename(control_frisbee_unique = control_fris_unique,
         control_bubblewrap_unique = control_bubb_unique,
         bubblewrap_self_creative = bubble_self_creative,
         bubblewrap_self_original = bubble_self_original, 
         bubblewrap_self_useful = bubble_self_useful,
         control_bubblewrap_self_creative = control_bubble_self_1,
         control_bubblewrap_self_original = control_bubble_self_2,
         control_bubblewrap_self_useful = control_bubble_self_3,
         control_frisbee_self_creative = control_frisbee_self_1,
         control_frisbee_self_original = control_frisbee_self_2,
         control_frisbee_self_useful = control_frisbee_self_3,
         control_brick_self_creative = control_brick_self_1,
         control_brick_self_original = control_brick_self_2,
         control_brick_self_useful = control_brick_self_3,
         Black = race_1,
         American_Indian = race_2,
         White = race_3,
         Native_Hawaiian = race_4,
         Hispanic_Latino = race_5,
         Asian = race_6,
         self_describe = race_7,
         unknown_race = race_8,
         submitter_id=ResponseId) %>% 
  group_by(submitter_id) %>% 
  
  # Step 1: Replace NA with 0 for ALL race columns at once
  # (Update this list with all your actual race column names)
  mutate(across(c(Black:unknown_race), ~replace_na(., 0))) %>% 
  
  # Step 2: Create a sum column to easily identify Multiracial participants
  mutate(race_sum = Black+American_Indian+White+Native_Hawaiian+Hispanic_Latino+Asian+self_describe+unknown_race) %>% 
  
  # Step 3: Assign the final text label
  mutate(race = case_when(
    race_sum > 1 ~ "Multiracial",   # IMPORTANT: This must be first!
    race_sum == 0 ~ "Unknown/Prefer not to say",
    Black == 1    ~ "Black",
    American_Indian == 1    ~ "American_Indian",
    White == 1    ~ "White",
    Native_Hawaiian == 1   ~ "Native_Hawaiian",
    Hispanic_Latino == 1    ~ "Hispanic_Latino",
    Asian == 1 ~ "Asian",
    self_describe == 1 ~ "self_describe",
    unknown_race == 1 ~ "unknown_race",
    TRUE          ~ "Error" # Failsafe
  ))
```

```{r}
trials <- cleaned_names %>% 
  pivot_longer(
    cols = c(
      matches("^ai_[a-zA-Z]+_[0-9]+$"), 
      matches("^control_[a-zA-Z]+_[0-9]+$")
    ),
    names_to = "trial",
    values_to = "use",
    # This regex looks for ai_ OR control_ and discards it, keeping the rest
    names_pattern = "^(?:ai|control)_(.*)$", 
    values_drop_na = TRUE
  ) %>% 
  pivot_longer(
    cols = matches("^(ai|control)_[a-zA-Z]+_selected$"),
    names_to = "selected_object",
    names_pattern = "^(?:ai|control)_(.*)_selected$",
    values_to = "selected_idea",
    values_drop_na = TRUE
  ) %>% 
  pivot_longer(
    cols = contains("self", ignore.case = FALSE),
    names_to = "selected_trial",
    values_to = "self_evaluation",
    values_drop_na = TRUE
  ) %>% 
  filter(str_detect(selected_trial, fixed(selected_object))) %>%  # Check if the objects are the same
  mutate(metric = str_extract(selected_trial, "creative|original|useful")) %>% 
  select(-c(FL_3_DO_FL_100:`FL_96_DO_ControlBubbleSelf-Rating`)) %>% 
  pivot_wider(id_cols = c(submitter_id, Condition, trial, use, selected_object, selected_idea, effort_1, age, gender, White, race),
              names_from = "metric",
              values_from = "self_evaluation") %>% 
  separate(col = "trial",
           into = c("object",
                    "trial_num")) 

demographics <- trials %>% 
  distinct(submitter_id, White, gender, age)
  
```

## Demographics

```{r}
cleaned_names %>% 
  get_summary_stats(age, type="mean_sd")


cleaned_names %>% 
  group_by(race) %>% 
  count()
```

```{r}
ideas_to_evaluate <- cleaned_names %>% 
  select(submitter_id, Condition, matches("(frisbee|bubblewrap|brick)_[0-9]+$")) %>% 
  ungroup() %>% 
  mutate(ID = row_number()) %>% 
  unite(col = "frisbee_1",
        ends_with("frisbee_1"),
        na.rm = TRUE) %>% 
  unite(col = "frisbee_2",
        ends_with("frisbee_2"),
        na.rm = TRUE) %>% 
  unite(col = "frisbee_3",
        ends_with("frisbee_3"),
        na.rm = TRUE) %>% 
  unite(col = "frisbee_4",
        ends_with("frisbee_4"),
        na.rm = TRUE) %>% 
  unite(col = "frisbee_5",
        ends_with("frisbee_5"),
        na.rm = TRUE) %>% 
  unite(col = "brick_1",
        ends_with("brick_1"),
        na.rm = TRUE) %>% 
  unite(col = "brick_2",
        ends_with("brick_2"),
        na.rm = TRUE) %>% 
  unite(col = "brick_3",
        ends_with("brick_3"),
        na.rm = TRUE) %>% 
  unite(col = "brick_4",
        ends_with("brick_4"),
        na.rm = TRUE) %>% 
  unite(col = "brick_5",
        ends_with("brick_5"),
        na.rm = TRUE) %>% 
  unite(col = "bubblewrap_1",
        ends_with("bubblewrap_1"),
        na.rm = TRUE) %>% 
  unite(col = "bubblewrap_2",
        ends_with("bubblewrap_2"),
        na.rm = TRUE) %>% 
  unite(col = "bubblewrap_3",
        ends_with("bubblewrap_3"),
        na.rm = TRUE) %>% 
  unite(col = "bubblewrap_4",
        ends_with("bubblewrap_4"),
        na.rm = TRUE) %>% 
  unite(col = "bubblewrap_5",
        ends_with("bubblewrap_5"),
        na.rm = TRUE) 


```

```{r}

df <- read_csv("data/cloudresearch_ideas.csv") 
  
# 1. Define the target number of subsets
# We know we have 1280 of each item, and we want 5 per subset.
# 1280 / 5 = 256 subsets.
# 2. Assign Subsets
n_subsets <- 256
n_items_per_subset <- 5

# Shuffle and assign IDs
df_processed <- df %>%
  group_by(prompt_object) %>%
  sample_frac(1) %>%  # Randomize order (shuffles Condition)
  mutate(
    # Assign subset IDs 1..256 repeatedly
    subset_id = rep(1:n_subsets, each = n_items_per_subset),
    # Create a rank 1..5 for pivoting
    item_rank = rep(1:n_items_per_subset, times = n_subsets)
  ) %>%
  ungroup()

df_processed_demographics <- df_processed %>% 
  full_join(demographics, by="submitter_id")
  
# 3. Pivot to Wide Format
master_df <- df_processed_demographics %>%
  pivot_wider(
    id_cols = subset_id,
    names_from = c(prompt_object, item_rank),
    values_from = c(idea_text, Condition, submitter_id, White, gender, age),
    names_glue = "{prompt_object}_{item_rank}_{.value}" # e.g., brick_1_idea_text
  ) %>%
  # Clean up column names to match your request (e.g., "brick_1")
  rename_with(~ str_remove(., "_idea_text"), ends_with("_idea_text"))

write_csv(master_df, "data/ideas_with_subsets.csv")
```

```{r}
# Write cleaned data to csv

write_csv(trials, "data/cloudresearch_2025.12.08.csv")

ideas_for_homogeneity <- trials %>% 
  distinct(submitter_id, Condition, object, use)
write_csv(ideas_for_homogeneity, "data/ideas_for_homogeneity_analysis.csv")

```

# External Raters 

This survey version didn't correctly export the data. So I tried again with a different survey. Still needed to compensate these 4 participants.

```{r}
external_ratings <- fetch_survey(surveyID = "SV_24x0YL5QZe75SZM",
                                 convert = FALSE,
                                 label = TRUE,
                                 breakout_sets = FALSE)

# write_csv(external_ratings, "data/external_ratings.csv")
```

##Crowdworker ratings
### 1st round

```{r}
ratings <- fetch_survey("SV_cU6nKz3X4RYuE18",
                             label = TRUE,
                             convert = FALSE,
                            breakout_sets = FALSE)

passed_attn_check <- ratings %>% 
  filter(attn_check == "Other" & attn_check_4_TEXT == 8) %>% 
  rename(subset_id = "subset_ids_DO")

attempted_raters <- read_csv("data/raters.csv") %>% 
  rename(participantId = "ParticipantId")

failed_attn_check <- attempted_raters %>% 
  anti_join(passed_attn_check, by="participantId") %>% 
  select(participantId) %>% 
  mutate(Message = "Sorry, your submission was incomplete!") %>% 
  mutate(`Reason For Rejection` = "Incomplete")

write_csv(failed_attn_check, "data/failed_attn_check.csv")

# Attention check 2
ratings %>% 
  select(ResponseId, attn_check_2) %>% 
  filter(is.na(attn_check_2))
```

## Check which idea subsets were rated
```{r}
passed_attn_check %>% 
  count(subset_ids_DO) %>% 
  print(n=204)

# Get the idea subsets that were not rated
unrated <- master_df %>% 
  anti_join(passed_attn_check)
write_csv(unrated, "data/unrated.csv")
```

### 2nd round
```{r}
# 2nd round of data collection
ratings_2 <- fetch_survey("SV_6uoub8m15d2YZy6",
                          label = TRUE,
                          convert = FALSE,
                          breakout_sets = FALSE)
  
```


```{r}
ratings_long <- ratings %>% 
  pivot_longer(cols =15:11534,
               names_to = "rater_trial",
               values_to = "rating") %>% 
  drop_na(rating) %>% 
  mutate(rating_num = as.numeric(str_extract(rating, "\\d"))) %>% 
  extract(
    col = rater_trial,
    into = c("trial_id", "metric"),
    # Regex Explanation:
    # ^(.*)    -> Capture everything from the start...
    # _        -> ...until the very last underscore...
    # ([^_]+)$ -> ...and capture the final chunk (the metric) separately.
    regex = "^(.*)_([^_]+)$"
  )
```


```{r}

# 1. Define the vector of new numbers (these were unrated in the 1st round of data collection)
new_numbers <- c(
  1, 11, 15, 17, 26, 27, 30, 34, 40, 41, 
  42, 43, 62, 69, 71, 76, 78, 84, 88, 92, 
  96, 105, 107, 113, 117, 120, 121, 123, 144, 149, 
  161, 167, 168, 170, 171, 175, 176, 181, 189, 190, 
  193, 205, 208, 211, 215, 217, 223, 225, 226, 230, 
  232, 250
)

# 2. Define the sequence of old numbers (incorrectly populated by Qualtrics Loop and Merge)
old_numbers <- 257:308

# Check: Ensure both lists are the same length (should be 52)
if(length(new_numbers) != length(old_numbers)) {
  stop("Error: The number of new IDs does not match the number of old IDs.")
}

# 3. Rename columns so they match the master df
for (i in seq_along(old_numbers)) {
  
  # Define the pattern: starts with (^) the old number
  pattern <- paste0("^", old_numbers[i])
  
  # Define the replacement: the corresponding new number
  replacement <- as.character(new_numbers[i])
  
  # Update column names
  names(ratings_2) <- sub(pattern, replacement, names(ratings_2))
}


ratings_2_long <- ratings_2 %>% 
  pivot_longer(cols =15:2354,
               names_to = "rater_trial",
               values_to = "rating") %>% 
  drop_na(rating) %>% 
  mutate(rating_num = as.numeric(str_extract(rating, "\\d"))) %>% 
  extract(
    col = rater_trial,
    into = c("trial_id", "metric"),
    # Regex Explanation:
    # ^(.*)    -> Capture everything from the start...
    # _        -> ...until the very last underscore...
    # ([^_]+)$ -> ...and capture the final chunk (the metric) separately.
    regex = "^(.*)_([^_]+)$"
  )
```

```{r}
# Merge rating data with the information about where the idea came from

decoder_key <- master_df %>%
  # --- Step A: Rename "Idea" columns to match the pattern ---
  # matches("^[a-z]+_\\d+$") finds columns like 'brick_1' but ignores 'brick_1_Condition'
  rename_with(
    .fn = ~paste0(., "_Text"), 
    .cols = matches("^[a-z]+_\\d+$")
  ) %>%
  
  # --- Step B: Pivot everything at once ---
  # We use a regex pattern to split the column names into 3 parts:
  # 1. Object (brick)  2. Digit (1)  3. Type (Text or Condition)
  pivot_longer(
    cols = -subset_id,
    names_to = c("object", "digit", ".value"),
    names_pattern = "(.*)_(\\d+)_(.*)"
  ) %>%
  
  # --- Step C: Create ID and Cleanup ---
  mutate(
    trial_id = paste(subset_id, object, digit, sep = "_")
  ) %>%
  select(trial_id, idea_text = Text, submitter_condition = Condition)


condition_ratings <- left_join(ratings_long, decoder_key, by="trial_id")

# 46 of the 52 remaining subset IDs have been rated
condition_ratings_2 <- left_join(ratings_2_long, decoder_key, by="trial_id")

# Bind rows
individual_ratings <- rbind(condition_ratings, condition_ratings_2) %>% 
  select(participantId, subset_ids_DO, trial_id:submitter_condition) %>% 
  rename(subset_id = subset_ids_DO)


```

```{r}
# Get the idea subsets that were still not rated after 2nd round
unrated_2 <- master_df %>% 
  anti_join(individual_ratings, by="subset_id")

write_csv(unrated_2, "data/unrated2.csv")
```

### 3rd round
```{r}
ratings_3 <- fetch_survey("SV_1NR8ooZcp8qTtZQ",
                          label = TRUE,
                          convert = FALSE,
                          breakout_sets = FALSE)

old_numbers <- 309:314
new_numbers <- c(15, 26, 88, 193, 211, 230)

# Check: Ensure both lists are the same length (should be 52)
if(length(new_numbers) != length(old_numbers)) {
  stop("Error: The number of new IDs does not match the number of old IDs.")
}

# 3. Rename columns so they match the master df
for (i in seq_along(old_numbers)) {
  
  # Define the pattern: starts with (^) the old number
  pattern <- paste0("^", old_numbers[i])
  
  # Define the replacement: the corresponding new number
  replacement <- as.character(new_numbers[i])
  
  # Update column names
  names(ratings_3) <- sub(pattern, replacement, names(ratings_3))
}

```
```{r}
ratings_3_long <- ratings_3 %>% 
  pivot_longer(cols =15:284,
               names_to = "rater_trial",
               values_to = "rating") %>% 
  drop_na(rating) %>% 
  mutate(rating_num = as.numeric(str_extract(rating, "\\d"))) %>% 
  extract(
    col = rater_trial,
    into = c("trial_id", "metric"),
    # Regex Explanation:
    # ^(.*)    -> Capture everything from the start...
    # _        -> ...until the very last underscore...
    # ([^_]+)$ -> ...and capture the final chunk (the metric) separately.
    regex = "^(.*)_([^_]+)$"
  )
```
```{r}
condition_ratings_3 <- left_join(ratings_3_long, decoder_key, by="trial_id") %>% 
  rename(subset_id = "subset_ids_DO") %>% 
  select(participantId, subset_id, trial_id:submitter_condition)

# Bind rows
all_ratings <- rbind(individual_ratings, condition_ratings_3) 
```

## Individual-Level Creativity
```{r}
df_processed_demographics <- df_processed_demographics %>% 
  unite(col = "trial_id",         # The name of the new column
        c(subset_id, original_col),  # The columns to combine
        sep = "_",                   # The separator (e.g., underscore, space, dash)
        remove = FALSE)
```

```{r}

plot_data <- all_ratings %>% 
  filter(metric %in% c("creative", "original", "useful")) %>% 
  group_by(metric, submitter_condition) %>% 
  get_summary_stats(rating_num, type = "mean_ci")

```


```{r}

all_ratings %>% 
  # 1. Aggregate to the person level
  # We group by ID, Metric, AND Condition to preserve the condition label
  group_by(subset_id, metric, submitter_condition) %>% 
  summarize(person_mean = mean(rating_num, na.rm = TRUE), .groups = "drop") %>% 
  
  # 2. Plot the distribution of the person-means
  ggplot(aes(x = submitter_condition, y = person_mean, fill = submitter_condition)) +
  geom_boxplot() +
  facet_wrap(~metric) +
  theme_minimal() +
  stat_summary(fun="mean") +
  labs(y = "Mean Rating (per person)") +
  scale_fill_manual(values = c("control" = "#619bff", "high-agency" = "#00BA38", "low-agency" = "#F8766D", "unstructured" = "gold")) 
```

```{r}
all_ratings_wide <- all_ratings %>% 
  select(-rating) %>% 
  pivot_wider(
              names_from = "metric",
              values_from = "rating_num") 

# Merge with submitters' demographic data
master_long <- master_df %>%
  pivot_longer(
    # Only pivot the columns that follow your trial pattern
    # (Safe if you have a participant ID column you don't want to mess up)
    cols = matches("brick|frisbee|bubblewrap"), 
    
    # '.value' tells R: "Use the second group to make new column headers"
    names_to = c("trial_id", ".value"), 
    
    # The Regex Magic:
    # ^([^_]+_[^_]+) -> Start at beginning, grab (text_text) as Group 1 (trial_id)
    # _              -> Skip the underscore separator
    # (.*)           -> Grab everything else as Group 2 (the column name)
    names_pattern = "^([^_]+_[^_]+)_(.*)"
  ) %>% 
  drop_na(trial_id) %>% 
  unite(col = "trial_id",
        c(subset_id, trial_id))

all_ratings_demographics <- left_join(all_ratings_wide, master_long, by="trial_id")

write_csv(master_df, "data/master_df.csv")
```

```{r}
individual_creativity_mod <- lmer(creative ~ submitter_condition + as.factor(White) + as.factor(gender) + scale(age) + (1 | submitter_id), data = all_ratings_demographics)
sjPlot::tab_model(individual_creativity_mod)

individual_originality_mod <- lmer(original ~ submitter_condition + as.factor(White) + as.factor(gender) + scale(age) + (1 | submitter_id), data = all_ratings_demographics)
sjPlot::tab_model(individual_originality_mod)

individual_usefulness_mod <- lmer(useful ~ submitter_condition + as.factor(White) + as.factor(gender) + scale(age) + (1 | submitter_id), data = all_ratings_demographics)
sjPlot::tab_model(individual_usefulness_mod)
```

## Homogeneity Analysis

```{r}
homogeneity <- read_csv("data/cloudresearch_homogeneity.csv")


homogeneity_covariates <- left_join(homogeneity, demographics, by="submitter_id")

homogeneity_model <- lmer(similarity ~ condition + scale(age) + as.factor(gender) + as.factor(White)+ (1|submitter_id) + (1|object), data=homogeneity_covariates)

summary(homogeneity_model)
anova(homogeneity_model)
```


###Doshi & Hauser analysis
```{r}
leave_one_out <- read_csv("data/doshi_analysis.csv")

leave_one_out_covariates <- left_join(leave_one_out, demographics, by="submitter_id")

leave_one_out_model <- lmer(sim_condition ~ Condition + (1|submitter_id) + (1|object), data=leave_one_out_covariates)

summary(leave_one_out_model)
anova(leave_one_out_model)
```

### Homogeneity Plot

```{r}
homogeneity_means_df <- homogeneity_covariates %>%
  group_by(condition) %>%
  summarise(mean_homogeneity = mean(similarity, na.rm = TRUE))

homogeneity_plot <- homogeneity_covariates %>% 
  mutate(condition= fct_relevel(condition, c("low-agency", "high-agency", "unstructured", "control"))) %>% 
  ggplot(aes(x=similarity, y=condition, fill=condition)) +
  ggridges::geom_density_ridges(alpha=.6,
                      quantile_lines = TRUE,
                      quantile_fun = mean) +
  geom_text(
    data = homogeneity_means_df,
    aes(x = mean_homogeneity, y = condition, 
        label = round(mean_homogeneity, 2)),   # label = numeric mean
    nudge_x = -0.05,    # shift to the right of the mean line
    nudge_y = 0.85,  
    size = 3.5,
    color = "black"
  ) +
  scale_x_continuous(breaks=seq(-.5, 1, by=.1)) +
  labs(x="Homogeneity",
       y="") +
  jtools::theme_apa() +
  theme(legend.position = "none") +
  scale_fill_manual(values = c("control" = "#619bff", "high-agency" = "#00BA38", "low-agency" = "#F8766D", "unstructured" = "gold")) 

homogeneity_plot
```

## Self-perceived creativity

```{r}
self_creativity_model <- lmer(creative ~ Condition + (1|submitter_id), data=trials)
summary(self_creativity_model)

self_originality_model <- lmer(original ~ Condition + (1|submitter_id), data=trials)
summary(self_originality_model)

self_usefulness_model <- lmer(useful ~ Condition + (1|submitter_id), data=trials)
summary(self_usefulness_model)
```

# Chat Data Analysis

```{r}
chat_data <- cleaned_names %>% 
  filter(Condition != "control") %>% 
  select(submitter_id, Condition, ends_with("chatMessageCount"), ends_with("totalPasteEvents")) %>% 
  rowwise() %>% 
  mutate(mean_turns = mean(c_across(q1_chatMessageCount:q3_chatMessageCount), na.rm=TRUE)) %>% 
  mutate(copy_paste = mean(c_across(q1_totalPasteEvents:q3_totalPasteEvents))) 

chat_data %>% 
  group_by(Condition) %>% 
  get_summary_stats(c(mean_turns, copy_paste), type="mean_se") %>% 
  ggplot(aes(x=Condition, y=mean, color=Condition)) +
  geom_point(size=3) +
  geom_errorbar(aes(ymin=mean-se, ymax=mean+se)) +
  facet_wrap(~variable) +
  scale_color_manual(values = c("high-agency" = "#00BA38", "low-agency" = "#F8766D", "unstructured" = "gold")) +
  jtools::theme_apa()


```

```{r}
chat_log <- cleaned_names %>% 
  select(submitter_id, Condition, q1_chatMessageCount:q3_totalPasteEvents)
write_csv(cleaned_names, "data/cloudresearch_chatlog.csv")
```

```{r}
# 1. Load the data
df <- read_csv("data/cloudresearch_chatlog.csv")

# 2. Reshape and Parse
parsed_df <- df %>%
  # Select the necessary columns
  select(submitter_id, Condition, ends_with("chatConversationLog"), ends_with("chatSessionDurationMs")) %>%
  
  # Pivot to a "long" format so all chat logs are in one column
  pivot_longer(
    cols = ends_with("chatConversationLog"),
    names_to = "source_column",
    values_to = "json_content"
  ) %>%
  
  # Remove empty rows
  filter(!is.na(json_content)) %>%
  
  # Parse the JSON string for each row
  mutate(parsed_data = map(json_content, function(x) {
    jsonlite::fromJSON(x) %>%
    as_tibble() %>%
  mutate(
    # Convert ISO 8601 string to POSIXct datetime object
    timestamp = ymd_hms(timestamp),
    
    # Optional: Convert types to factors for easier grouping later
    type = factor(type, levels = c("session_start", "user_message", "ai_response"))
  ) %>%
  # Good practice to ensure chronological order
  arrange(timestamp)
  })) %>%
  
  # Expand the list column into actual dataframe rows
  unnest(parsed_data) %>%
  
  # Remove the original raw JSON string column
  select(-json_content) %>% 
  filter(type != "session_start")

# 3. View and Save
print(head(parsed_df))


parsed_chats <- parsed_df %>% 
  mutate(length = str_count(content, "\\w+"))

parsed_chats %>% 
  filter(type=="user_message") %>% 
  group_by(Condition) %>% 
  get_summary_stats(length, type="mean_se") %>% 
  ggplot(aes(x=Condition, y=mean)) +
  geom_point(size=3) +
  geom_errorbar(aes(ymin=mean-se, ymax=mean+se)) +
  labs(y="Mean number of words in user message") +
  ylim(c(5, 15)) +
  jtools::theme_apa()

write_csv(parsed_df, "data/parsed_chats.csv")
```
