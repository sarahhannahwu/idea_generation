---
title: "Pilot: Idea Generation with AI"
format: 
  html:
    code-overflow: wrap
    code-fold: true
    df-print: paged
    embed-resources: true
    css: |
      body {
        max-width: 1200px;
        word-wrap: break-word;
      }
      pre code {
        white-space: pre-wrap;
      }
editor: visual
jupyter: python3
---

## Libraries

```{r, echo=FALSE}
library(here) # to enable easy file referencing
library(renv) # helps create reproducible environments
library(tidyverse)
library(dplyr)
library(text)
library(descriptr)
library(ggsignif)
library(ggpubr)
library(scales)
library(lme4)
library(lmerTest)
library(simr)
library(broom)
library(kableExtra)
library(knitr)
library(papaja) # For automatic formatting of statistics
library(progress)
library(emmeans)
library(ggrepel)
library(RColorBrewer)


```

### Read the data

```{r}
# Remove question text rows
pilot_data <- read_csv("data/pilot_2025.09.11.csv") %>% 
  slice(-(1:2))

# Filter for participants who passed attention check
passed_attn_check <- pilot_data %>% 
  filter(attn_check == "4" & attn_check_4_TEXT == "8")



```

### Basic cleaning, memory check results

Near the end of the survey, we asked participants to recall the gist of the instruction text for the condition to which they were assigned. They picked from the multiple choice options of "Stay in control of the AI tool" (Pilot), "Let the AI tool do the heavy lifting" (Passenger), or "Don't use AI" (Control).

38 out of 45 participants correctly recalled the gist of the instruction text. The 6 of the 7 participants who did not correctly recall the instruction text incorrectly selected "Stay in control of the AI tool," suggesting that the options may have been unclear.

```{r}
# Rename condition column

pilot_data_renamed <- passed_attn_check %>% 
  rename(condition = FL_3_DO) %>% 
  relocate(condition, .after = attn_check_4_TEXT) %>% 
  mutate(condition = case_when(condition == "GuidedInstructions" ~ "Pilot",
                               condition == "UnguidedInstructions" ~ "Passenger",
                               condition == "ControlInstructions" ~ "Control"))

# Filter for participants who correctly answered the memory check question about the condition to which they were assigned
# 1 = "Stay in control of the AI tool" (Pilot)
# 2 = "Let the AI tool do the heavy lifting (Passenger)
# 3 = "Don't use AI" (Control)

memory_check <- pilot_data_renamed %>% 
  mutate(memory_check_condition = case_when(memory_check == "1" ~ "Pilot",
                                            memory_check == "2" ~ "Passenger",
                                            is.na(memory_check) ~ "Control")) %>% 
  relocate(memory_check_condition, .after = "memory_check") %>% 
  mutate(memory_check_result = if_else(condition == memory_check_condition, 1, 0)) %>% 
  relocate(memory_check_result, .after = "memory_check_condition")



# Convert to long format
trial_data <- pilot_data_renamed %>% 
  pivot_longer(names_to = "trial",
                values_to = "use",
                cols = starts_with(c("pilot", "pass", "control"))) %>% 
  mutate(trial = str_remove(trial, "open_")) %>% 
  filter(!is.na(use)) %>% 
  relocate(trial, .after = "condition") %>% 
  relocate(use, .after = "trial") %>% 
  mutate(object = str_extract(trial, "(?<=_).*")) %>% 
  mutate(object = str_remove(object, "_.*")) %>% 
  relocate(object, .after = "trial") %>% 
  mutate(object = if_else(object == "bubble", "bubblewrap", object))

write_csv(trial_data, "data/trial_data.csv")
```

## Run ocsai model to automatically score originality

```{r}
originality <- read_csv("data/trial_data_scored.csv")

# Add a column with trial number
originality_trials <- originality %>% 
  mutate(trial_num = str_extract(trial, "(?<=_)[^_]*$")) %>% 
  relocate(trial_num, .after = "trial")

# Originality by condition
ggplot(originality, aes(x=condition, y=originality)) +
  geom_boxplot() +
  stat_summary(fun="mean")

# Plot originality over the course of the study
originality_trials %>% 
  group_by(condition, trial_num) %>% 
  summarize(originality = mean(originality)) %>% 
  ggplot(aes(x=trial_num, y = originality, group=condition, color=condition)) +
  geom_line() +
  geom_point() +
  labs(x="Trial Number") +
  ylim(1,5)

```

In this small sample of N=15/condition, the Passenger condition had the most original responses (as scored by the model), followed by the Control condition, and then the Pilot condition.

## Homogeneity analysis using sentence embedding model

```{r}
options(digits=3)

# Group the uses by condition and object 
trial_data %>% 
  select(ResponseId, condition, object, use) %>% 
  group_by(condition, object) %>% 
  arrange(condition, object) 

# Use sentence transformers model in Python to analyze semantic similarity
semantic_similarities <- read_csv("data/semantic_similarities.csv")
semantic_similarities_summary <- read_csv("data/semantic_similarity_summary.csv")

semantic_similarities %>% 
  group_by(condition) %>% 
  summarize(mean_similarity = mean(similarity),
            sd_similarity = sd(similarity)) %>% 
  arrange(desc(mean_similarity))

ggboxplot(semantic_similarities, x="condition", y="similarity", color="condition") +
  stat_summary(fun="mean") 
```

The Passenger condition had the greatest semantic similarity among ideas, followed by the Pilot condition, and then Control condition.

## Crowdsourced evaluations of creativity

The next step will be to have another set of crowdworkers evaluate the creativity of the pilot participants' ideas in terms of overall creativity, originality, and usefulness.

```{r}

# Prepare idea submissions dataset for evaluation by another group of crowdworkers
# The data needs to be in long format, where each row represents a single idea for a single object

# In case you want to view wide
idea_submissions_wide <- trial_data %>% 
  mutate(trial = str_extract(trial, "([A-Za-z0-9]+)(_)([A-Za-z0-9]+)$")) %>% 
  pivot_wider(id_cols = c("ResponseId", "condition", "process_open-ended"),
              names_from = "trial",
              values_from = "use")

idea_submissions_long <- trial_data %>% 
  unite(col = "idea_id", ResponseId, trial, sep="-", remove = FALSE)
  

write_csv(idea_submissions_long, "data/idea_submissions.csv")

# Create separate csvs for frisbee, brick, and bubblewrap
frisbee_submissions <- idea_submissions_long %>% 
  filter(object=="frisbee")

brick_submissions <- idea_submissions_long %>% 
  filter(object=="brick")

bubble_submissions <- idea_submissions_long %>% 
  filter(object=="bubblewrap")

# Save to csvs

write_csv(frisbee_submissions, "data/frisbee_submissions.csv")
write_csv(brick_submissions, "data/brick_submissions.csv")
write_csv(bubble_submissions, "data/bubble_submissions.csv")
           
```

### Analysis of crowdworkers' subjective evaluations

N = 99 participants

```{r}
# Read in the crowdworker evaluations
# Each row represents a crowdworkers' responses. The data is super wide because each column represents a response to a single pilot study participant's idea
# First, wrangle the data so that the column format is similar to the pilot study data: frisbee_1, frisbee_2...bubble_5

crowdworkers <- read_csv("data/crowdworker_eval_2025.09.18.csv")

# Check which idea submitter is tied to which crowdworker
submitter_ids <- crowdworkers %>% 
  slice_head(n=1) %>% 
  pivot_longer(cols = 14:2038, 
               names_to = "participant_trial",
               values_to = "submitter_id") %>% 
  select(participant_trial, submitter_id) %>% 
  mutate(submitter_id = str_extract(submitter_id, "R_[A-Za-z0-9]+")) %>% 
  separate(participant_trial,
           into = c("person_id", "trial"),
           extra = "merge") %>% 
    separate(col = trial,
           into = c("metric", "trial"),
           sep = "_",
           extra = "merge") %>% 
  mutate(metric = case_when(metric == "c" ~ "creativity",
                            metric == "o" ~ "originality",
                            metric == "u" ~ "usefulness")) %>% 
  distinct(person_id, submitter_id)


crowdworker_eval <- crowdworkers %>% 
  filter(DistributionChannel=="anonymous") 

crowdworker_passed_attn_check <- crowdworker_eval %>% 
  filter(attn_check == "4" & attn_check_4_TEXT == "8")


```

```{r}
# Ensure there is a column for the ID of the pilot participant that the submitted idea came from
crowdworker_eval_long <- crowdworker_passed_attn_check %>% 
  pivot_longer(cols = 14:2038,
               names_to = "participant_trial",
               values_to = "rating") %>% 
  drop_na(rating) %>% 
  separate(col = participant_trial, 
           into = c("person_id", "trial"),
           sep = "_",
           extra = "merge") %>% 
  mutate(rating = as.numeric(rating)) %>% 
  separate(col = trial,
           into = c("metric", "trial"),
           sep = "_",
           extra = "merge") %>% 
  mutate(metric = case_when(metric == "c" ~ "creativity",
                            metric == "o" ~ "originality",
                            metric == "u" ~ "usefulness")) %>% 
  unite(col="participant_rating_id", ResponseId, metric, trial, remove = FALSE) %>% 
  mutate(trial_id = str_remove(participant_rating_id, "_creativity|_originality|_usefulness")) %>% 
  pivot_wider(id_cols=c(trial_id, person_id), 
              names_from = "metric",
              values_from = "rating")


crowdworkers_submitters <- left_join(crowdworker_eval_long, submitter_ids, by = "person_id") %>% 
  extract(trial_id, 
          into = c("crowdworker_id", "trial"),
          regex = "(R_[A-Za-z0-9]+)_(.*)")




```

```{r}
# Merge with trial data from the pilot
# participantID refers to the CloudResearch ID of the idea submitter

trial_data_renamed <- trial_data %>% 
  rename(submitter_id = "ResponseId") %>% 
  mutate(trial = str_remove(trial, "^[^_]*_")) %>% 
  select(submitter_id, condition, trial, object, use, participantId)

crowdworkers_submitters_ideas <- full_join(crowdworkers_submitters, trial_data_renamed, by=c("submitter_id", "trial"))

# Descriptives of overall creativity, originality, and usefulness by condition
crowdworkers_submitters_ideas %>% 
  group_by(condition) %>% 
  get_summary_stats(c(creativity, originality, usefulness),
                    type="mean_sd") %>% 
  arrange(variable)


```

### Compensate bonuses for those whose ideas were rated as greater than 3 out of 5 on the creativity scales

7 out of the 45 pilot study participants met the criteria for a bonus.

```{r}
bonuses <- crowdworkers_submitters_ideas %>% 
  group_by(participantId) %>% 
  get_summary_stats(c(creativity, originality, usefulness),
                    type="mean_sd") %>% 
  pivot_wider(id_cols = "participantId",
              names_from = "variable",
              values_from = "mean") %>% 
  filter(originality > 3 & usefulness > 3) %>% 
  select(-c(creativity, originality, usefulness)) %>% 
  rename("Participant or Assignment" = participantId) %>% 
  mutate(Amount = 1.00,
         Message = "Great job! Your ideas scored high on creativity, so you receive a bonus.") 

write_csv(bonuses, "data/pilot_bonus.csv")
  

```

```{r}
crowdworkers_submitters_ideas %>% 
  pivot_longer(cols=c(creativity, originality, usefulness),
               names_to = "metric",
               values_to = "rating") %>% 
  ggplot(aes(x=condition, y=rating)) +
  geom_boxplot() +
  stat_summary(fun="mean") +
  facet_wrap(~metric)

```

```{r}
# Distributions of ratings for each condition
crowdworkers_submitters_ideas %>% 
  pivot_longer(cols=c(creativity, originality, usefulness),
               names_to = "metric",
               values_to = "rating") %>% 
  ggplot(aes(x=rating)) +
  geom_histogram(bins=5,
                 color="white",
                 aes(y=after_stat(density))) +
  facet_wrap(~metric+condition)

```

```{r}
# For each pair of submitter_id and crowdworker_id we can compute mean ratings
crowdworkers_submitters_ideas %>% 
  group_by(submitter_id, crowdworker_id) %>% 
  summarize(mean_creativity = mean(creativity),
         mean_originality = mean(originality),
         mean_usefulness = mean(usefulness)) %>% 
  group_by(submitter_id) %>% 
  mutate(rater_num = row_number()) %>% 
  ungroup() 



# What is the relationship between originality and overall creativity? Usefulness and overall creativity? Interaction between the two?


# Comparison of automated vs. manual originality evaluations
manual_ratings <- crowdworkers_submitters_ideas %>% 
  group_by(submitter_id) %>% 
  summarize(mean_creativity = mean(creativity),
         mean_originality = mean(originality),
         mean_usefulness = mean(usefulness)) %>% 
  rename(manual_originality = "mean_originality")

automated_ratings <- originality_trials %>% 
  group_by(ResponseId) %>% 
  summarize(originality = mean(originality)) %>% 
  rename(submitter_id = "ResponseId",
         automated_originality = "originality") 

cor.test(manual_ratings$manual_originality, automated_ratings$automated_originality)

both_ratings <- full_join(manual_ratings, automated_ratings, by="submitter_id")

ggplot(both_ratings, aes(x=manual_originality, y=automated_originality)) +
  geom_point()


```

## Similarity Model

```{r}
# originality_mod <- lm(originality ~ )
# 
# kable(tidy(mod), digits=2)
# 
# tstats <- t.test()

semantic_similarity_mod <- lmer(similarity ~ condition + (1|object) + (1|ResponseId), data=semantic_similarities)
summary(semantic_similarity_mod)
```

Examine differences (`r apa_print(tstats)$statistic`).

Template to render in APA style can be found here: https://wjschne.github.io/apaquarto/installation.html

# Exploratory Next Steps

## Chat log data: Actual interactions with AI

We want to compare the interaction behaviors of Passengers and Pilots. We expect that Passengers will copy-paste more than Pilots. Generally, their submitted ideas should look more like what ChatGPT suggested. We can also look at the prompting strategies of Passengers vs. Pilots.

```{r}
chat_log <- read_csv("data/cleaned_user_ai_conversations.csv")
```

```{r}


```

## Process data as a moderator of creative outcomes

We asked participants to report the extent to which they (1) felt they were engaging in dialogue with AI, (2) felt that AI helped augment their creativity, and (3) found the task intrinsically motivating. We might expect, for example, that Pilots produce more diverse ideas *only if* they perceive that AI helped them consider unexpected angles of the object.

Regardless of experimental condition, we might find systematic differences in AI usage when we compare the most vs. least creative participants. How can we characterize those differences?

# Power Analysis 

I tried a simple power analysis in GPower, but that wasn't the best for my data and study design. So now I'm using a simulation:

```{r, echo=FALSE}
# Create a mixed model from pilot data
manual_ratings_model <- lmer(
  originality ~ condition + (1|object) + (1|submitter_id), data=crowdworkers_submitters_ideas)
summary(manual_ratings_model)

# # Decide how many submitters
unique_submitters <- unique(crowdworkers_submitters_ideas$submitter_id)
#   
# # Decide how many raters needed to evaluate those 
# unique_raters <- unique(crowdworkers_submitters_ideas$crowdworker_id)
# unique_raters_sample <- sample(unique_raters, size = S, replace = TRUE)

# Sample submitters
# Get all of their ideas
# Rename their ideas so they're unique
# Combine into simulation dataset
# Repeat until you have the number of ratings specified in N

# Determine total number of ratings
# Then figure out how those ratings ought to be sourced. How many submitters does that require, holding the number of ideas and objects constant.
# Account for presentation order after the fact, rather than constraining the study design.

# Loop through the sample of submitters
sims <- 1000
S <- 300 # This can be adjusted to test different numbers of submitters

originality_summaries <- list()
for (i in 1:sims) {
  # data_resamp <- crowdworkers_submitters_ideas %>%
  # slice_sample(n = N, replace = TRUE)
  
  unique_submitters_sample <- sample(unique_submitters, size = S, replace = TRUE)
  
  data_sample_list <- list()
  for (j in 1:length(unique_submitters_sample)){
    data_sample_list[[j]] <- crowdworkers_submitters_ideas %>% 
      filter(submitter_id == unique_submitters_sample[j]) %>% 
      mutate(submitter_id = paste0(submitter_id, j)) 
    
  }
  data_resamp <- do.call(rbind, data_sample_list)
  manual_ratings_model <- lmer(
  originality ~ condition + (1|object) + (1|submitter_id), data=data_resamp)
  originality_summaries[[i]] <- summary(manual_ratings_model)$coefficients[2:3, 3]
}

originality_summaries_bound <- do.call(rbind, originality_summaries)

# t must be greater than 2 to be significant. We just need absolute value

power_Passenger <- mean(abs(summaries_bound[,1]) > 2)
power_Pilot <- mean(abs(summaries_bound[,2]) > 2)

power_Passenger
power_Pilot
  

```

```{r, echo=FALSE}
# # Create simulated model using extracted parameters
# model <- makeLmer(originality ~ condition + (1|trial), fixef=fixed, VarCorr=rand, sigma=s, data=originality_trials)
# model
# 
# # Extend the model along ResponseId
# model_ext <- extend(model, along="trial", n=500)
# model_ext
```

## IRiSS Sample

```{r}
iriss <- read_csv("data/iriss.csv") %>% 
  slice(-(1:2)) %>% 
  filter(attn_check == "4" & attn_check_4_TEXT == "8") %>% 
  rename(condition = FL_3_DO) %>% 
  relocate(condition, .after = attn_check_4_TEXT) %>% 
  mutate(condition = case_when(condition == "GuidedInstructions" ~ "Pilot",
                               condition == "UnguidedInstructions" ~ "Passenger",
                               condition == "ControlInstructions" ~ "Control")) %>% 
  filter(Progress=="100")

iriss_memory_check <- iriss %>% 
  mutate(memory_check_condition = case_when(memory_check == "1" ~ "Pilot",
                                            memory_check == "2" ~ "Passenger",
                                            is.na(memory_check) ~ "Control")) %>% 
  relocate(memory_check_condition, .after = "memory_check") %>% 
  mutate(memory_check_result = if_else(condition == memory_check_condition, 1, 0)) %>% 
  relocate(memory_check_result, .after = "memory_check_condition") 

# Check the % of participants who remembered what the instructions said
iriss_memory_check %>% 
  count(memory_check_result) %>% 
  mutate(prop=n/sum(n))

iriss_memory_check %>% 
  count(condition)


```
```{r}
# Convert to long format
iriss_trial_data <- iriss %>% 
  pivot_longer(names_to = "trial",
                values_to = "use",
                cols = starts_with(c("pilot", "pass", "control"))) %>% 
  mutate(trial = str_remove(trial, "open_")) %>% # fixed naming problem
  filter(grepl("^(pilot|pass|control)_(brick|frisbee|bubble)_[1-5]$", trial)) %>% 
  filter(!is.na(use)) %>% # Keep only the trials with responses
  relocate(trial, .after = "condition") %>% 
  relocate(use, .after = "trial") %>% 
  mutate(object = str_extract(trial, "(?<=_).*")) %>% 
  mutate(object = str_remove(object, "_.*")) %>% 
  relocate(object, .after = "trial") %>% 
  mutate(object = if_else(object == "bubble", "bubblewrap", object))

write_csv(iriss_trial_data, "data/iriss_trial_data.csv")
```


```{r}
iriss_self_perceptions <- iriss %>% 
  pivot_longer(names_to = "metric",
               values_to = "rating",
               cols = c(ends_with("_unique"),
                        matches("self_[1-3]$"))) %>% 
  drop_na(rating) %>% 
  mutate(metric = str_replace_all(metric, 
                                  c("self_1" = "creativity",
                                    "self_2" = "originality",
                                    "self_3" = "usefulness"))) %>% 
  separate(
    col = metric,
    into = c("condition", "object", "question"),
    sep = "_",
    remove = TRUE# keep original column if you still want it
  ) %>% 
  mutate(
    object = str_replace_all(
      object,
      c("\\bfris\\b" = "frisbee",
        "\\bbubb\\b"  = "bubble"))) %>% 
  mutate(condition = case_when(condition == "pilot" ~ "Pilot",
                               condition == "pass" ~ "Passenger",
                               condition == "control" ~ "Control")) %>% 
  pivot_wider(names_from = "question",
              values_from = "rating") %>% 
  mutate(across(c(creativity, originality, usefulness), as.numeric)) 

```


```{r}
# How unique did participants think their ideas were?
iriss_self_perceptions %>% 
  drop_na(unique) %>% 
  ggplot(aes(x=unique)) +
  geom_bar(position = "dodge") +
  facet_wrap(~condition) +
  scale_x_discrete(labels = c(
    "1" = "No one",
    "2" = "Some people",
    "3" = "Most people",
    "4" = "Everyone"
  )) +
  labs(title="Do you think many other people came up with this idea as well?")
```


```{r}
# How does mean originality compare across conditions?

iriss_self_perceptions %>% 
  drop_na(originality) %>% 
  group_by(ResponseId, condition) %>% 
  summarize(originality = mean(originality)) %>% 
  ggplot(aes(x=condition, y=originality)) +
  geom_boxplot() +
  stat_summary(fun="mean") 
  labs(y="Self-perceived originality")
```


```{r}
iriss_self_originality_mod <- lmer(
  originality ~ condition + (1|object) + (1|ResponseId), data= iriss_self_perceptions)
summary(iriss_self_originality_mod)
```


```{python}
from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"


# Load the sentence transformer model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Load the data

data_path = '~/Git_Projects/idea_generation/data/iriss_trial_data.csv'  # Update with your actual data path
df = pd.read_csv(data_path)

# For each combination of experimental condition, object, and ResponseId, compute the semantic similarity of ideas
results = []
for (condition, obj, response_id), group in df.groupby(['condition', 'object', 'ResponseId']):
    ideas = group['use'].tolist()

    # Compute embeddings
    embeddings = model.encode(ideas)

    # Compute cosine similarity matrix
    norm_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
    similarity_matrix = np.dot(norm_embeddings, norm_embeddings.T)

    # Extract upper triangle of the similarity matrix, excluding the diagonal
    upper_tri_indices = np.triu_indices_from(similarity_matrix, k=1)
    similarities = similarity_matrix[upper_tri_indices]

    # Store results
    for sim in similarities:
        results.append({
            'condition': condition,
            'object': obj,
            'ResponseId': response_id,
            'similarity': sim
        })

# Convert results to DataFrame
results_df = pd.DataFrame(results)
summary = results_df.groupby('condition')['similarity'].agg(['mean', 'std'])
print(summary)

# Save results to CSV
output_path = os.path.expanduser('~/Git_Projects/idea_generation/data/iriss_semantic_similarities.csv')  # Update with your desired output path
results_df.to_csv(output_path, index=False)
```

```{r}
iriss_semantic_similarities <- read_csv("data/iriss_semantic_similarities.csv")
iriss_semantic_similarity_mod <- lmer(similarity ~ condition + (1|object) + (1|ResponseId), data=iriss_semantic_similarities)
summary(iriss_semantic_similarity_mod)
```
According to the linear mixed model, Passengers produce significantly more similar ideas than the Control group. 

```{r}
iriss_semantic_similarities %>% 
  group_by(condition, ResponseId) %>% 
  summarize(similarity = mean(similarity)) %>% 
  ggplot(aes(x=condition, y=similarity)) +
  geom_boxplot() +
  stat_summary(fun="mean") 
```
```{r}
emmeans(iriss_semantic_similarity_mod, pairwise ~ condition)
```

## Crowdsourced originality ratings
```{r}
iriss_manual_ratings <- read_csv("data/iriss_manual_ratings.csv") %>% 
  filter(DistributionChannel=="anonymous") %>% 
  filter(attn_check == "4" & attn_check_4_TEXT == "8") %>% 
  drop_na(frisbee.0.condition) # Drop rows with missing data (experimenter error) 


iriss_manual_ratings_long <- iriss_manual_ratings %>% 
  rename(rater_id = ResponseId) %>% 
  pivot_longer(cols = frisbee.0.condition:bubblewrap.4.use,
               names_to = "stimulus_metadata",
               values_to = "value") %>% 
  mutate(metadata = str_extract(stimulus_metadata, "[^.]+$"),
         stimulus_metadata = str_remove(stimulus_metadata, "\\.[^.]+$")) %>% 
  separate_wider_delim(stimulus_metadata,
           ".",
           names = c("object", "trial_num")) %>% 
  pivot_wider(
              names_from = "metadata",
              values_from = "value") %>% 
  rename(submitter_id = ResponseId)

# Pivot to give each creativity rating its own column
iriss_manual_ratings_metrics <- iriss_manual_ratings_long %>% 
  pivot_longer(cols = c_frisbee_1:u_bubble_5,
               names_to = "trial",
               values_to = "rating") %>% 
  mutate(rating = as.numeric(rating)) %>% 
  mutate(metric = str_extract(trial, "^[A-Za-z]+(?=_)")) %>% 
  mutate(metric = case_when(metric == "c" ~ "creativity",
                            metric == "o" ~ "originality",
                            metric == "u" ~ "usefulness")) %>% 
  select(rater_id, object, trial_num, submitter_id, condition, use, trial, metric, rating) %>% 
  mutate(trial = str_remove(trial, "^._")) %>% 
  mutate(trial_num = as.numeric(trial_num),
         trial_num = trial_num+1) %>% 
  unite(col="stimulus", c("object", "trial_num")) %>% 
  mutate(trial = str_replace_all(trial, "bubble", "bubblewrap")) %>% # Check names!
  filter(stimulus==trial) %>% 
  pivot_wider(names_from = "metric",
              values_from = "rating") %>% 
  mutate(object = str_extract(stimulus, "^[A-Za-z]+")) %>% 
  relocate(object, .after = "stimulus") %>% 
  mutate(use_word_count = str_count(use, "\\S+")) %>% 
  relocate(use_word_count, .after = use)

```

### Descriptives
```{r}
iriss_manual_ratings_metrics %>% 
  group_by(condition) %>% 
  get_summary_stats(use_word_count, type="mean_sd")

iriss_manual_ratings_metrics %>% 
  ggplot(aes(x=condition, y=use_word_count)) +
  geom_boxplot() +
  stat_summary(fun="mean")
```

### Originality analysis
```{r}
iriss_manual_ratings_metrics %>% 
  group_by(condition) %>%
  get_summary_stats(c(creativity, originality, usefulness), type="mean_sd") %>% 
  arrange(variable)

iriss_manual_ratings_metrics %>% 
  group_by(condition) %>% 
  count(originality) %>% 
  mutate(prop=n/sum(n)) %>% 
  ggplot(aes(x=originality, y=prop)) +
  geom_col(color="white") +
  facet_wrap(~condition)

label_points <- iriss_manual_ratings_metrics %>% 
  sample_n(21)

ggplot(iriss_manual_ratings_metrics, aes(x=originality, y=usefulness, fill=creativity)) +
  geom_tile(color="white") +
  geom_label_repel(
    data=label_points,
    aes(label = str_wrap(paste(object, use, sep = ": "), width = 20),
        color = condition),
    size=2.5) +
  theme_minimal()

```
```{r}
iriss_originality_mod <- lmer(
  originality ~ condition + (1|object) + (1|submitter_id), data= iriss_manual_ratings_metrics)
summary(iriss_originality_mod)

# Repeat models for creativity and usefulness
iriss_creativity_mod <- lmer(
  creativity ~ condition + (1|object) + (1|submitter_id), data= iriss_manual_ratings_metrics)
summary(iriss_creativity_mod)

iriss_usefulness_mod <- lmer(
  usefulness ~ condition + (1|object) + (1|submitter_id), data= iriss_manual_ratings_metrics)
summary(iriss_usefulness_mod)
```

