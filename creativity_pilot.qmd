---
title: "Pilot: Idea Generation with AI"
format: 
  html:
    code-overflow: wrap
    code-fold: true
    df-print: paged
    embed-resources: true
    css: |
      body {
        max-width: 1200px;
        word-wrap: break-word;
      }
      pre code {
        white-space: pre-wrap;
      }
editor: visual
---

## Libraries

```{r, echo=FALSE}
library(here) # to enable easy file referencing
library(renv) # helps create reproducible environments
library(tidyverse)
library(dplyr)
library(text)
library(descriptr)
library(ggsignif)
library(ggpubr)
library(scales)
library(lme4)
library(simr)
library(broom)
library(kableExtra)
library(knitr)
library(papaja) # For automatic formatting of statistics
library(progress)


```

## Read the data

```{r}
# Remove question text rows
pilot_data <- read_csv("data/pilot_2025.09.11.csv") %>% 
  slice(-(1:2))

# Filter for participants who passed attention check
passed_attn_check <- pilot_data %>% 
  filter(attn_check == "4" & attn_check_4_TEXT == "8")



```

## Basic cleaning, memory check results

Near the end of the survey, we asked participants to recall the gist of the instruction text for the condition to which they were assigned. They picked from the multiple choice options of "Stay in control of the AI tool" (Pilot), "Let the AI tool do the heavy lifting" (Passenger), or "Don't use AI" (Control).

38 out of 45 participants correctly recalled the gist of the instruction text. The 6 of the 7 participants who did not correctly recall the instruction text incorrectly selected "Stay in control of the AI tool," suggesting that the options may have been unclear.

```{r}
# Rename condition column

pilot_data_renamed <- passed_attn_check %>% 
  rename(condition = FL_3_DO) %>% 
  relocate(condition, .after = attn_check_4_TEXT) %>% 
  mutate(condition = case_when(condition == "GuidedInstructions" ~ "Pilot",
                               condition == "UnguidedInstructions" ~ "Passenger",
                               condition == "ControlInstructions" ~ "Control"))

# Filter for participants who correctly answered the memory check question about the condition to which they were assigned
# 1 = "Stay in control of the AI tool" (Pilot)
# 2 = "Let the AI tool do the heavy lifting (Passenger)
# 3 = "Don't use AI" (Control)

memory_check <- pilot_data_renamed %>% 
  mutate(memory_check_condition = case_when(memory_check == "1" ~ "Pilot",
                                            memory_check == "2" ~ "Passenger",
                                            is.na(memory_check) ~ "Control")) %>% 
  relocate(memory_check_condition, .after = "memory_check") %>% 
  mutate(memory_check_result = if_else(condition == memory_check_condition, 1, 0)) %>% 
  relocate(memory_check_result, .after = "memory_check_condition")



# Convert to long format
trial_data <- pilot_data_renamed %>% 
  pivot_longer(names_to = "trial",
                values_to = "use",
                cols = starts_with(c("pilot", "pass", "control"))) %>% 
  mutate(trial = str_remove(trial, "open_")) %>% 
  filter(!is.na(use)) %>% 
  relocate(trial, .after = "condition") %>% 
  relocate(use, .after = "trial") %>% 
  mutate(object = str_extract(trial, "(?<=_).*")) %>% 
  mutate(object = str_remove(object, "_.*")) %>% 
  relocate(object, .after = "trial") %>% 
  mutate(object = if_else(object == "bubble", "bubblewrap", object))

write_csv(trial_data, "data/trial_data.csv")
```

```{r}

```

## Run ocsai model to automatically score originality

```{r}
originality <- read_csv("data/trial_data_scored.csv")

# Add a column with trial number
originality_trials <- originality %>% 
  mutate(trial_num = str_extract(trial, "(?<=_)[^_]*$")) %>% 
  relocate(trial_num, .after = "trial")

# Originality by condition
ggplot(originality, aes(x=condition, y=originality)) +
  geom_boxplot() +
  stat_summary(fun="mean")

# Plot originality over the course of the study
originality_trials %>% 
  group_by(condition, trial_num) %>% 
  summarize(originality = mean(originality)) %>% 
  ggplot(aes(x=trial_num, y = originality, group=condition, color=condition)) +
  geom_line() +
  geom_point() +
  labs(x="Trial Number") +
  ylim(1,5)

```

In this small sample of N=15/condition, the Passenger condition had the most original responses (as scored by the model), followed by the Control condition, and then the Pilot condition.

## Homogeneity analysis using sentence embedding model

```{r}
options(digits=3)

# Group the uses by condition and object 
trial_data %>% 
  select(ResponseId, condition, object, use) %>% 
  group_by(condition, object) %>% 
  arrange(condition, object) 

# Use sentence transformers model in Python to analyze semantic similarity
semantic_similarities <- read_csv("data/semantic_similarities.csv")
semantic_similarities_summary <- read_csv("data/semantic_similarity_summary.csv")

semantic_similarities %>% 
  group_by(condition) %>% 
  summarize(mean_similarity = mean(similarity),
            sd_similarity = sd(similarity)) %>% 
  arrange(desc(mean_similarity))

ggboxplot(semantic_similarities, x="condition", y="similarity", color="condition") +
  stat_summary(fun="mean") 
```

The Passenger condition had the greatest semantic similarity among ideas, followed by the Pilot condition, and then Control condition.

## Crowdsourced evaluations of creativity

The next step will be to have another set of crowdworkers evaluate the creativity of the pilot participants' ideas in terms of overall creativity, originality, and usefulness.

```{r}

# Prepare idea submissions dataset for evaluation by another group of crowdworkers
# The data needs to be in long format, where each row represents a single idea for a single object

# In case you want to view wide
idea_submissions_wide <- trial_data %>% 
  mutate(trial = str_extract(trial, "([A-Za-z0-9]+)(_)([A-Za-z0-9]+)$")) %>% 
  pivot_wider(id_cols = c("ResponseId", "condition", "process_open-ended"),
              names_from = "trial",
              values_from = "use")

idea_submissions_long <- trial_data %>% 
  unite(col = "idea_id", ResponseId, trial, sep="-")
  

write_csv(idea_submissions_long, "data/idea_submissions.csv")

# Create separate csvs for frisbee, brick, and bubblewrap
frisbee_submissions <- idea_submissions_long %>% 
  filter(object=="frisbee")

brick_submissions <- idea_submissions_long %>% 
  filter(object=="brick")

bubble_submissions <- idea_submissions_long %>% 
  filter(object=="bubblewrap")

# Save to csvs

write_csv(frisbee_submissions, "data/frisbee_submissions.csv")
write_csv(brick_submissions, "data/brick_submissions.csv")
write_csv(bubble_submissions, "data/bubble_submissions.csv")
           
```

## Analysis of crowdworkers' subjective evaluations

N = 99 participants

```{r}
# Read in the crowdworker evaluations
# Each row represents a crowdworkers' responses. The data is super wide because each column represents a response to a single pilot study participant's idea
# First, wrangle the data so that the column format is similar to the pilot study data: frisbee_1, frisbee_2...bubble_5

crowdworkers <- read_csv("data/crowdworker_eval_2025.09.18.csv")

# Check which idea submitter is tied to which crowdworker
submitter_ids <- crowdworkers %>% 
  slice_head(n=1) %>% 
  pivot_longer(cols = 14:2038, 
               names_to = "participant_trial",
               values_to = "submitter_id") %>% 
  select(participant_trial, submitter_id) %>% 
  mutate(submitter_id = str_extract(submitter_id, "R_[A-Za-z0-9]+")) %>% 
  separate(participant_trial,
           into = c("person_id", "trial"),
           extra = "merge") %>% 
    separate(col = trial,
           into = c("metric", "trial"),
           sep = "_",
           extra = "merge") %>% 
  mutate(metric = case_when(metric == "c" ~ "creativity",
                            metric == "o" ~ "originality",
                            metric == "u" ~ "usefulness")) %>% 
  distinct(person_id, submitter_id)


crowdworker_eval <- crowdworkers %>% 
  filter(DistributionChannel=="anonymous") 

crowdworker_passed_attn_check <- crowdworker_eval %>% 
  filter(attn_check == "4" & attn_check_4_TEXT == "8")


```

```{r}
# Ensure there is a column for the ID of the pilot participant that the submitted idea came from
crowdworker_eval_long <- crowdworker_passed_attn_check %>% 
  pivot_longer(cols = 14:2038,
               names_to = "participant_trial",
               values_to = "rating") %>% 
  drop_na(rating) %>% 
  separate(col = participant_trial, 
           into = c("person_id", "trial"),
           sep = "_",
           extra = "merge") %>% 
  mutate(rating = as.numeric(rating)) %>% 
  separate(col = trial,
           into = c("metric", "trial"),
           sep = "_",
           extra = "merge") %>% 
  mutate(metric = case_when(metric == "c" ~ "creativity",
                            metric == "o" ~ "originality",
                            metric == "u" ~ "usefulness")) %>% 
  unite(col="participant_rating_id", ResponseId, metric, trial, remove = FALSE) %>% 
  mutate(trial_id = str_remove(participant_rating_id, "_creativity|_originality|_usefulness")) %>% 
  pivot_wider(id_cols=c(trial_id, person_id), 
              names_from = "metric",
              values_from = "rating")


crowdworkers_submitters <- left_join(crowdworker_eval_long, submitter_ids, by = "person_id") %>% 
  extract(trial_id, 
          into = c("crowdworker_id", "trial"),
          regex = "(R_[A-Za-z0-9]+)_(.*)")




```

```{r}
# Merge with trial data from the pilot
# participantID refers to the CloudResearch ID of the idea submitter

trial_data_renamed <- trial_data %>% 
  rename(submitter_id = "ResponseId") %>% 
  mutate(trial = str_remove(trial, "^[^_]*_")) %>% 
  select(submitter_id, condition, trial, object, use, participantId)

crowdworkers_submitters_ideas <- full_join(crowdworkers_submitters, trial_data_renamed, by=c("submitter_id", "trial"))

# Descriptives of overall creativity, originality, and usefulness by condition
crowdworkers_submitters_ideas %>% 
  group_by(condition) %>% 
  get_summary_stats(c(creativity, originality, usefulness),
                    type="mean_sd") %>% 
  arrange(variable)


```

## Compensate bonuses for those whose ideas were rated as greater than 3 out of 5 on the creativity scales

7 out of the 45 pilot study participants met the criteria for a bonus.

```{r}
bonuses <- crowdworkers_submitters_ideas %>% 
  group_by(participantId) %>% 
  get_summary_stats(c(creativity, originality, usefulness),
                    type="mean_sd") %>% 
  pivot_wider(id_cols = "participantId",
              names_from = "variable",
              values_from = "mean") %>% 
  filter(originality > 3 & usefulness > 3) %>% 
  select(-c(creativity, originality, usefulness)) %>% 
  rename("Participant or Assignment" = participantId) %>% 
  mutate(Amount = 1.00,
         Message = "Great job! Your ideas scored high on creativity, so you receive a bonus.") 

write_csv(bonuses, "data/pilot_bonus.csv")
  

```

```{r}
crowdworkers_submitters_ideas %>% 
  pivot_longer(cols=c(creativity, originality, usefulness),
               names_to = "metric",
               values_to = "rating") %>% 
  ggplot(aes(x=condition, y=rating)) +
  geom_boxplot() +
  stat_summary(fun="mean") +
  facet_wrap(~metric)

```

```{r}
# Distributions of ratings for each condition
crowdworkers_submitters_ideas %>% 
  pivot_longer(cols=c(creativity, originality, usefulness),
               names_to = "metric",
               values_to = "rating") %>% 
  ggplot(aes(x=rating)) +
  geom_histogram(bins=5,
                 color="white",
                 aes(y=after_stat(density))) +
  facet_wrap(~metric+condition)

```

```{r}
# For each pair of submitter_id and crowdworker_id we can compute mean ratings
crowdworkers_submitters_ideas %>% 
  group_by(submitter_id, crowdworker_id) %>% 
  summarize(mean_creativity = mean(creativity),
         mean_originality = mean(originality),
         mean_usefulness = mean(usefulness)) %>% 
  group_by(submitter_id) %>% 
  mutate(rater_num = row_number()) %>% 
  ungroup() 



# What is the relationship between originality and overall creativity? Usefulness and overall creativity? Interaction between the two?


# Comparison of automated vs. manual originality evaluations
manual_ratings <- crowdworkers_submitters_ideas %>% 
  group_by(submitter_id) %>% 
  summarize(mean_creativity = mean(creativity),
         mean_originality = mean(originality),
         mean_usefulness = mean(usefulness)) %>% 
  rename(manual_originality = "mean_originality")

automated_ratings <- originality_trials %>% 
  group_by(ResponseId) %>% 
  summarize(originality = mean(originality)) %>% 
  rename(submitter_id = "ResponseId",
         automated_originality = "originality") 

cor.test(manual_ratings$manual_originality, automated_ratings$automated_originality)

both_ratings <- full_join(manual_ratings, automated_ratings, by="submitter_id")

ggplot(both_ratings, aes(x=manual_originality, y=automated_originality)) +
  geom_point()


```

## Table of Regression Outputs
```{r}
# originality_mod <- lm(originality ~ )
# 
# kable(tidy(mod), digits=2)
# 
# tstats <- t.test()


```


The difference was significant (`r apa_print(tstats)$statistic`).

Template to render in APA style can be found here: https://wjschne.github.io/apaquarto/installation.html

# Exploratory Next Steps

## Chat log data: Actual interactions with AI

We want to compare the interaction behaviors of Passengers and Pilots. We expect that Passengers will copy-paste more than Pilots. Generally, their submitted ideas should look more like what ChatGPT suggested. We can also look at the prompting strategies of Passengers vs. Pilots.

```{r}
chat_log <- read_csv("data/cleaned_user_ai_conversations.csv")
```


```{r}


```

## Process data as a moderator of creative outcomes

We asked participants to report the extent to which they (1) felt they were engaging in dialogue with AI, (2) felt that AI helped augment their creativity, and (3) found the task intrinsically motivating. We might expect, for example, that Pilots produce more diverse ideas *only if* they perceive that AI helped them consider unexpected angles of the object.

Regardless of experimental condition, we might find systematic differences in AI usage when we compare the most vs. least creative participants. How can we characterize those differences?

# Power Analysis for Study 2

I tried a simple power analysis in GPower, but that wasn't the best for my data and study design. So now I'm using simr:

```{r, echo=FALSE}
# Create a mixed model from pilot data
manual_ratings_model <- lmer(
  originality ~ condition + (1|trial) + (1|submitter_id), data=crowdworkers_submitters_ideas)
summary(manual_ratings_model)

# # Decide how many submitters
unique_submitters <- unique(crowdworkers_submitters_ideas$submitter_id)
#   
# # Decide how many raters needed to evaluate those 
# unique_raters <- unique(crowdworkers_submitters_ideas$crowdworker_id)
# unique_raters_sample <- sample(unique_raters, size = S, replace = TRUE)

# Sample submitters
# Get all of their ideas
# Rename their ideas so they're unique
# Combine into simulation dataset
# Repeat until you have the number of ratings specified in N

# Determine total number of ratings
# Then figure out how those ratings ought to be sourced. How many submitters does that require, holding the number of ideas and objects constant.
# Account for presentation order after the fact, rather than constraining the study design.

# Loop through the sample of submitters
sims <- 1000
S <- 300 # This can be adjusted to test different numbers of submitters

summaries <- list()
for (i in 1:sims) {
  # data_resamp <- crowdworkers_submitters_ideas %>%
  # slice_sample(n = N, replace = TRUE)
  
  unique_submitters_sample <- sample(unique_submitters, size = S, replace = TRUE)
  
  data_sample_list <- list()
  for (j in 1:length(unique_submitters_sample)){
    data_sample_list[[j]] <- crowdworkers_submitters_ideas %>% 
      filter(submitter_id == unique_submitters_sample[j]) %>% 
      mutate(submitter_id = paste0(submitter_id, j)) 
    
  }
  data_resamp <- do.call(rbind, data_sample_list)
  manual_ratings_model <- lmer(
  originality ~ condition + (1|trial) + (1|submitter_id), data=data_resamp)
  summaries[[i]] <- summary(manual_ratings_model)$coefficients[2:3, 3]
}

summaries_bound <- do.call(rbind, summaries)

# t must be greater than 2 to be significant. We just need absolute value

power_Passenger <- mean(abs(summaries_bound[,1]) > 2)
power_Pilot <- mean(abs(summaries_bound[,2]) > 2)

power_Passenger
power_Pilot
  

```

```{r, echo=FALSE}
# # Create simulated model using extracted parameters
# model <- makeLmer(originality ~ condition + (1|trial), fixef=fixed, VarCorr=rand, sigma=s, data=originality_trials)
# model
# 
# # Extend the model along ResponseId
# model_ext <- extend(model, along="trial", n=500)
# model_ext
```

```{}
```
