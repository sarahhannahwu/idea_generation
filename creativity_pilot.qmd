---
title: "Idea Generation with AI"
format: 
  html:
    code-overflow: wrap
    code-fold: true
    df-print: paged
    embed-resources: true
    css: |
      body {
        max-width: 1200px;
        word-wrap: break-word;
      }
      pre code {
        white-space: pre-wrap;
      }
editor: visual
jupyter: python3
---

## Libraries

```{r, echo=FALSE}
library(here) # to enable easy file referencing
library(renv) # helps create reproducible environments
library(tidyverse)
library(dplyr)
library(text)
library(descriptr)
library(ggsignif)
library(ggpubr)
library(scales)
library(lme4)
library(lmerTest)
library(simr)
library(broom)
library(kableExtra)
library(knitr)
library(papaja) # For automatic formatting of statistics
library(progress)
library(emmeans)
library(ggrepel)
library(RColorBrewer)
library(ggridges)
library(sjPlot)
library(webshot)
library(effectsize)
library(lavaan)
library(patchwork)
library(gtsummary)
library(sjstats)
library(ltm)
library(wCorr)
library(rstatix)
library(pwr)
library(lavaan)
library(mediation)
library(QuantPsyc)
library(gt)


```

### Read the data

```{r}
# Remove question text rows
pilot_data <- read_csv("data/pilot_2025.09.11.csv") %>%
  slice(-(1:2))

# Filter for participants who passed attention check
passed_attn_check <- pilot_data %>% 
  filter(attn_check == "4" & attn_check_4_TEXT == "8")



```

### Basic cleaning, memory check results

Near the end of the survey, we asked participants to recall the gist of the instruction text for the condition to which they were assigned. They picked from the multiple choice options of "Stay in control of the AI tool" (high-agency), "Let the AI tool do the heavy lifting" (low-agency), or "Don't use AI" (Control).

38 out of 45 participants correctly recalled the gist of the instruction text. The 6 of the 7 participants who did not correctly recall the instruction text incorrectly selected "Stay in control of the AI tool," suggesting that the options may have been unclear.

```{r}
# Rename condition column

pilot_data_renamed <- passed_attn_check %>% 
  rename(condition = FL_3_DO) %>% 
  relocate(condition, .after = attn_check_4_TEXT) %>% 
  mutate(condition = case_when(condition == "GuidedInstructions" ~ "high-agency",
                               condition == "UnguidedInstructions" ~ "low-agency",
                               condition == "ControlInstructions" ~ "Control"))

# Filter for participants who correctly answered the memory check question about the condition to which they were assigned
# 1 = "Stay in control of the AI tool" (high-agency)
# 2 = "Let the AI tool do the heavy lifting (low-agency)
# 3 = "Don't use AI" (Control)

memory_check <- high-agency_data_renamed %>% 
  mutate(memory_check_condition = case_when(memory_check == "1" ~ "high-agency",
                                            memory_check == "2" ~ "low-agency",
                                            is.na(memory_check) ~ "Control")) %>% 
  relocate(memory_check_condition, .after = "memory_check") %>% 
  mutate(memory_check_result = if_else(condition == memory_check_condition, 1, 0)) %>% 
  relocate(memory_check_result, .after = "memory_check_condition")



# Convert to long format
trial_data <- pilot_data_renamed %>% 
  pivot_longer(names_to = "trial",
                values_to = "use",
                cols = starts_with(c("high-agency", "pass", "control"))) %>% 
  mutate(trial = str_remove(trial, "open_")) %>% 
  filter(!is.na(use)) %>% 
  relocate(trial, .after = "condition") %>% 
  relocate(use, .after = "trial") %>% 
  mutate(object = str_extract(trial, "(?<=_).*")) %>% 
  mutate(object = str_remove(object, "_.*")) %>% 
  relocate(object, .after = "trial") %>% 
  mutate(object = if_else(object == "bubble", "bubblewrap", object))

write_csv(trial_data, "data/trial_data.csv")
```

## Run ocsai model to automatically score originality

```{r}
originality <- read_csv("data/trial_data_scored.csv")

# Add a column with trial number
originality_trials <- originality %>% 
  mutate(trial_num = str_extract(trial, "(?<=_)[^_]*$")) %>% 
  relocate(trial_num, .after = "trial")

# Originality by condition
ggplot(originality, aes(x=condition, y=originality)) +
  geom_boxplot() +
  stat_summary(fun="mean")

# Plot originality over the course of the study
originality_trials %>% 
  group_by(condition, trial_num) %>% 
  summarize(originality = mean(originality)) %>% 
  ggplot(aes(x=trial_num, y = originality, group=condition, color=condition)) +
  geom_line() +
  geom_point() +
  labs(x="Trial Number") +
  ylim(1,5)

```

In this small sample of N=15/condition, the low-agency condition had the most original responses (as scored by the model), followed by the Control condition, and then the high-agency condition.

## Homogeneity analysis using sentence embedding model

```{r}
options(digits=3)

# Group the uses by condition and object 
trial_data %>% 
  select(ResponseId, condition, object, use) %>% 
  group_by(condition, object) %>% 
  arrange(condition, object) 

# Use sentence transformers model in Python to analyze semantic similarity
semantic_similarities <- read_csv("data/semantic_similarities.csv") 
semantic_similarities_summary <- read_csv("data/semantic_similarity_summary.csv")

semantic_similarities %>% 
  group_by(condition) %>% 
  summarize(mean_similarity = mean(similarity),
            sd_similarity = sd(similarity)) %>% 
  arrange(desc(mean_similarity))

ggboxplot(semantic_similarities, x="condition", y="similarity", color="condition") +
  stat_summary(fun="mean") 
```

The low-agency condition had the greatest semantic similarity among ideas, followed by the high-agency condition, and then Control condition.

## Crowdsourced evaluations of creativity

The next step will be to have another set of crowdworkers evaluate the creativity of the high-agency participants' ideas in terms of overall creativity, originality, and usefulness.

```{r}

# Prepare idea submissions dataset for evaluation by another group of crowdworkers
# The data needs to be in long format, where each row represents a single idea for a single object

# In case you want to view wide
idea_submissions_wide <- trial_data %>% 
  mutate(trial = str_extract(trial, "([A-Za-z0-9]+)(_)([A-Za-z0-9]+)$")) %>% 
  pivot_wider(id_cols = c("ResponseId", "condition", "process_open-ended"),
              names_from = "trial",
              values_from = "use")

idea_submissions_long <- trial_data %>% 
  unite(col = "idea_id", ResponseId, trial, sep="-", remove = FALSE)
  

write_csv(idea_submissions_long, "data/idea_submissions.csv")

# Create separate csvs for frisbee, brick, and bubblewrap
frisbee_submissions <- idea_submissions_long %>% 
  filter(object=="frisbee")

brick_submissions <- idea_submissions_long %>% 
  filter(object=="brick")

bubble_submissions <- idea_submissions_long %>% 
  filter(object=="bubblewrap")

# Save to csvs

write_csv(frisbee_submissions, "data/frisbee_submissions.csv")
write_csv(brick_submissions, "data/brick_submissions.csv")
write_csv(bubble_submissions, "data/bubble_submissions.csv")
           
```

### Analysis of crowdworkers' subjective evaluations

N = 99 participants

```{r}
# Read in the crowdworker evaluations
# Each row represents a crowdworkers' responses. The data is super wide because each column represents a response to a single high-agency study participant's idea
# First, wrangle the data so that the column format is similar to the high-agency study data: frisbee_1, frisbee_2...bubble_5

crowdworkers <- read_csv("data/crowdworker_eval_2025.09.18.csv")

# Check which idea submitter is tied to which crowdworker
submitter_ids <- crowdworkers %>% 
  slice_head(n=1) %>% 
  pivot_longer(cols = 14:2038, 
               names_to = "participant_trial",
               values_to = "submitter_id") %>% 
  select(participant_trial, submitter_id) %>% 
  mutate(submitter_id = str_extract(submitter_id, "R_[A-Za-z0-9]+")) %>% 
  separate(participant_trial,
           into = c("person_id", "trial"),
           extra = "merge") %>% 
    separate(col = trial,
           into = c("metric", "trial"),
           sep = "_",
           extra = "merge") %>% 
  mutate(metric = case_when(metric == "c" ~ "creativity",
                            metric == "o" ~ "originality",
                            metric == "u" ~ "usefulness")) %>% 
  distinct(person_id, submitter_id)


crowdworker_eval <- crowdworkers %>% 
  filter(DistributionChannel=="anonymous") 

crowdworker_passed_attn_check <- crowdworker_eval %>% 
  filter(attn_check == "4" & attn_check_4_TEXT == "8")


```

```{r}
# Ensure there is a column for the ID of the high-agency participant that the submitted idea came from
crowdworker_eval_long <- crowdworker_passed_attn_check %>% 
  pivot_longer(cols = 14:2038,
               names_to = "participant_trial",
               values_to = "rating") %>% 
  drop_na(rating) %>% 
  separate(col = participant_trial, 
           into = c("person_id", "trial"),
           sep = "_",
           extra = "merge") %>% 
  mutate(rating = as.numeric(rating)) %>% 
  separate(col = trial,
           into = c("metric", "trial"),
           sep = "_",
           extra = "merge") %>% 
  mutate(metric = case_when(metric == "c" ~ "creativity",
                            metric == "o" ~ "originality",
                            metric == "u" ~ "usefulness")) %>% 
  unite(col="participant_rating_id", ResponseId, metric, trial, remove = FALSE) %>% 
  mutate(trial_id = str_remove(participant_rating_id, "_creativity|_originality|_usefulness")) %>% 
  pivot_wider(id_cols=c(trial_id, person_id), 
              names_from = "metric",
              values_from = "rating")


crowdworkers_submitters <- left_join(crowdworker_eval_long, submitter_ids, by = "person_id") %>% 
  extract(trial_id, 
          into = c("crowdworker_id", "trial"),
          regex = "(R_[A-Za-z0-9]+)_(.*)")




```

```{r}
# Merge with trial data from the high-agency
# participantID refers to the CloudResearch ID of the idea submitter

trial_data_renamed <- trial_data %>% 
  rename(submitter_id = "ResponseId") %>% 
  mutate(trial = str_remove(trial, "^[^_]*_")) %>% 
  select(submitter_id, condition, trial, object, use, participantId)

crowdworkers_submitters_ideas <- full_join(crowdworkers_submitters, trial_data_renamed, by=c("submitter_id", "trial"))

# Descriptives of overall creativity, originality, and usefulness by condition
crowdworkers_submitters_ideas %>% 
  group_by(condition) %>% 
  get_summary_stats(c(creativity, originality, usefulness),
                    type="mean_sd") %>% 
  arrange(variable)


```

### Compensate bonuses for those whose ideas were rated as greater than 3 out of 5 on the creativity scales

7 out of the 45 high-agency study participants met the criteria for a bonus.

```{r}
bonuses <- crowdworkers_submitters_ideas %>% 
  group_by(participantId) %>% 
  get_summary_stats(c(creativity, originality, usefulness),
                    type="mean_sd") %>% 
  pivot_wider(id_cols = "participantId",
              names_from = "variable",
              values_from = "mean") %>% 
  filter(originality > 3 & usefulness > 3) %>% 
  select(-c(creativity, originality, usefulness)) %>% 
  rename("Participant or Assignment" = participantId) %>% 
  mutate(Amount = 1.00,
         Message = "Great job! Your ideas scored high on creativity, so you receive a bonus.") 

write_csv(bonuses, "data/high-agency_bonus.csv")
  

```

```{r}
crowdworkers_submitters_ideas %>% 
  pivot_longer(cols=c(creativity, originality, usefulness),
               names_to = "metric",
               values_to = "rating") %>% 
  ggplot(aes(x=condition, y=rating)) +
  geom_boxplot() +
  stat_summary(fun="mean") +
  facet_wrap(~metric)

```

```{r}
# Distributions of ratings for each condition
crowdworkers_submitters_ideas %>% 
  pivot_longer(cols=c(creativity, originality, usefulness),
               names_to = "metric",
               values_to = "rating") %>% 
  ggplot(aes(x=rating)) +
  geom_histogram(bins=5,
                 color="white",
                 aes(y=after_stat(density))) +
  facet_wrap(~metric+condition)

```

```{r}
# For each pair of submitter_id and crowdworker_id we can compute mean ratings
crowdworkers_submitters_ideas %>% 
  group_by(submitter_id, crowdworker_id) %>% 
  summarize(mean_creativity = mean(creativity),
         mean_originality = mean(originality),
         mean_usefulness = mean(usefulness)) %>% 
  group_by(submitter_id) %>% 
  mutate(rater_num = row_number()) %>% 
  ungroup() 



# What is the relationship between originality and overall creativity? Usefulness and overall creativity? Interaction between the two?


# Comparison of automated vs. manual originality evaluations
manual_ratings <- crowdworkers_submitters_ideas %>% 
  group_by(submitter_id) %>% 
  summarize(mean_creativity = mean(creativity),
         mean_originality = mean(originality),
         mean_usefulness = mean(usefulness)) %>% 
  rename(manual_originality = "mean_originality")

automated_ratings <- originality_trials %>% 
  group_by(ResponseId) %>% 
  summarize(originality = mean(originality)) %>% 
  rename(submitter_id = "ResponseId",
         automated_originality = "originality") 

cor.test(manual_ratings$manual_originality, automated_ratings$automated_originality)

both_ratings <- full_join(manual_ratings, automated_ratings, by="submitter_id")

ggplot(both_ratings, aes(x=manual_originality, y=automated_originality)) +
  geom_point()


```

## Similarity Model

```{r}
# originality_mod <- lm(originality ~ )
# 
# kable(tidy(mod), digits=2)
# 
# tstats <- t.test()

semantic_similarity_mod <- lmer(similarity ~ condition + (1|object) + (1|ResponseId), data=semantic_similarities)
summary(semantic_similarity_mod)
```

Examine differences (`r apa_print(tstats)$statistic`).

Template to render in APA style can be found here: https://wjschne.github.io/apaquarto/installation.html

# Exploratory Next Steps

## Chat log data: Actual interactions with AI

We want to compare the interaction behaviors of low-agencys and high-agencys. We expect that low-agencys will copy-paste more than high-agencys. Generally, their submitted ideas should look more like what ChatGPT suggested. We can also look at the prompting strategies of low-agencys vs. high-agencys.

```{r}
chat_log <- read_csv("data/cleaned_user_ai_conversations.csv")
```

```{r}


```

## Process data as a moderator of creative outcomes

We asked participants to report the extent to which they (1) felt they were engaging in dialogue with AI, (2) felt that AI helped augment their creativity, and (3) found the task intrinsically motivating. We might expect, for example, that high-agencys produce more diverse ideas *only if* they perceive that AI helped them consider unexpected angles of the object.

Regardless of experimental condition, we might find systematic differences in AI usage when we compare the most vs. least creative participants. How can we characterize those differences?

# Power Analysis

I tried a simple power analysis in GPower, but that wasn't the best for my data and study design. So now I'm using a simulation:

```{r, echo=FALSE}
# Create a mixed model from high-agency data
manual_ratings_model <- lmer(
  originality ~ condition + (1|object) + (1|submitter_id), data=crowdworkers_submitters_ideas)
summary(manual_ratings_model)

# # Decide how many submitters
unique_submitters <- unique(crowdworkers_submitters_ideas$submitter_id)
#   
# # Decide how many raters needed to evaluate those 
# unique_raters <- unique(crowdworkers_submitters_ideas$crowdworker_id)
# unique_raters_sample <- sample(unique_raters, size = S, replace = TRUE)

# Sample submitters
# Get all of their ideas
# Rename their ideas so they're unique
# Combine into simulation dataset
# Repeat until you have the number of ratings specified in N

# Determine total number of ratings
# Then figure out how those ratings ought to be sourced. How many submitters does that require, holding the number of ideas and objects constant.
# Account for presentation order after the fact, rather than constraining the study design.

# Loop through the sample of submitters
sims <- 1000
S <- 130 # This can be adjusted to test different numbers of submitters

originality_summaries <- list()
for (i in 1:sims) {
  # data_resamp <- crowdworkers_submitters_ideas %>%
  # slice_sample(n = N, replace = TRUE)
  
  unique_submitters_sample <- sample(unique_submitters, size = S, replace = TRUE)
  
  data_sample_list <- list()
  for (j in 1:length(unique_submitters_sample)){
    data_sample_list[[j]] <- crowdworkers_submitters_ideas %>% 
      filter(submitter_id == unique_submitters_sample[j]) %>% 
      mutate(submitter_id = paste0(submitter_id, j)) 
    
  }
  data_resamp <- do.call(rbind, data_sample_list)
  manual_ratings_model <- lmer(
  originality ~ condition + (1|object) + (1|submitter_id), data=data_resamp)
  originality_summaries[[i]] <- summary(manual_ratings_model)$coefficients[2:3, 4] # Extract t-values, which are in the fourth column
}

originality_summaries_bound <- do.call(rbind, originality_summaries)

# t must be greater than 2 to be significant. We just need absolute value

power_low-agency <- mean(abs(originality_summaries_bound[,1]) > 2)
power_high-agency <- mean(abs(originality_summaries_bound[,2]) > 2)

power_low-agency
power_high-agency
  

```

```{r, echo=FALSE}
# # Create simulated model using extracted parameters
# model <- makeLmer(originality ~ condition + (1|trial), fixef=fixed, VarCorr=rand, sigma=s, data=originality_trials)
# model
# 
# # Extend the model along ResponseId
# model_ext <- extend(model, along="trial", n=500)
# model_ext
```
