---
title: "Idea Generation with AI"
format: 
  html:
    code-overflow: wrap
    code-fold: true
    df-print: paged
    embed-resources: true
    css: |
      body {
        max-width: 1200px;
        word-wrap: break-word;
      }
      pre code {
        white-space: pre-wrap;
      }
editor: visual
jupyter: python3
---

## Libraries

```{r, echo=FALSE}
library(here) # to enable easy file referencing
library(renv) # helps create reproducible environments
library(tidyverse)
library(dplyr)
library(text)
library(descriptr)
library(ggsignif)
library(ggpubr)
library(scales)
library(lme4)
library(lmerTest)
library(simr)
library(broom)
library(kableExtra)
library(knitr)
library(papaja) # For automatic formatting of statistics
library(progress)
library(emmeans)
library(ggrepel)
library(RColorBrewer)
library(ggridges)
library(sjPlot)
library(webshot)
library(effectsize)
library(lavaan)
library(patchwork)
library(gtsummary)
library(sjstats)
library(ltm)
library(wCorr)
library(rstatix)
library(pwr)
library(lavaan)
library(mediation)
library(QuantPsyc)
library(gt)


```

### Read the data

```{r}
# Remove question text rows
pilot_data <- read_csv("data/pilot_2025.09.11.csv") %>%
  slice(-(1:2))

# Filter for participants who passed attention check
passed_attn_check <- pilot_data %>% 
  filter(attn_check == "4" & attn_check_4_TEXT == "8")



```

### Basic cleaning, memory check results

Near the end of the survey, we asked participants to recall the gist of the instruction text for the condition to which they were assigned. They picked from the multiple choice options of "Stay in control of the AI tool" (high-agency), "Let the AI tool do the heavy lifting" (low-agency), or "Don't use AI" (Control).

38 out of 45 participants correctly recalled the gist of the instruction text. The 6 of the 7 participants who did not correctly recall the instruction text incorrectly selected "Stay in control of the AI tool," suggesting that the options may have been unclear.

```{r}
# Rename condition column

pilot_data_renamed <- passed_attn_check %>% 
  rename(condition = FL_3_DO) %>% 
  relocate(condition, .after = attn_check_4_TEXT) %>% 
  mutate(condition = case_when(condition == "GuidedInstructions" ~ "high-agency",
                               condition == "UnguidedInstructions" ~ "low-agency",
                               condition == "ControlInstructions" ~ "Control"))

# Filter for participants who correctly answered the memory check question about the condition to which they were assigned
# 1 = "Stay in control of the AI tool" (high-agency)
# 2 = "Let the AI tool do the heavy lifting (low-agency)
# 3 = "Don't use AI" (Control)

memory_check <- high-agency_data_renamed %>% 
  mutate(memory_check_condition = case_when(memory_check == "1" ~ "high-agency",
                                            memory_check == "2" ~ "low-agency",
                                            is.na(memory_check) ~ "Control")) %>% 
  relocate(memory_check_condition, .after = "memory_check") %>% 
  mutate(memory_check_result = if_else(condition == memory_check_condition, 1, 0)) %>% 
  relocate(memory_check_result, .after = "memory_check_condition")



# Convert to long format
trial_data <- pilot_data_renamed %>% 
  pivot_longer(names_to = "trial",
                values_to = "use",
                cols = starts_with(c("high-agency", "pass", "control"))) %>% 
  mutate(trial = str_remove(trial, "open_")) %>% 
  filter(!is.na(use)) %>% 
  relocate(trial, .after = "condition") %>% 
  relocate(use, .after = "trial") %>% 
  mutate(object = str_extract(trial, "(?<=_).*")) %>% 
  mutate(object = str_remove(object, "_.*")) %>% 
  relocate(object, .after = "trial") %>% 
  mutate(object = if_else(object == "bubble", "bubblewrap", object))

write_csv(trial_data, "data/trial_data.csv")
```

## Run ocsai model to automatically score originality

```{r}
originality <- read_csv("data/trial_data_scored.csv")

# Add a column with trial number
originality_trials <- originality %>% 
  mutate(trial_num = str_extract(trial, "(?<=_)[^_]*$")) %>% 
  relocate(trial_num, .after = "trial")

# Originality by condition
ggplot(originality, aes(x=condition, y=originality)) +
  geom_boxplot() +
  stat_summary(fun="mean")

# Plot originality over the course of the study
originality_trials %>% 
  group_by(condition, trial_num) %>% 
  summarize(originality = mean(originality)) %>% 
  ggplot(aes(x=trial_num, y = originality, group=condition, color=condition)) +
  geom_line() +
  geom_point() +
  labs(x="Trial Number") +
  ylim(1,5)

```

In this small sample of N=15/condition, the low-agency condition had the most original responses (as scored by the model), followed by the Control condition, and then the high-agency condition.

## Homogeneity analysis using sentence embedding model

```{r}
options(digits=3)

# Group the uses by condition and object 
trial_data %>% 
  select(ResponseId, condition, object, use) %>% 
  group_by(condition, object) %>% 
  arrange(condition, object) 

# Use sentence transformers model in Python to analyze semantic similarity
semantic_similarities <- read_csv("data/semantic_similarities.csv") 
semantic_similarities_summary <- read_csv("data/semantic_similarity_summary.csv")

semantic_similarities %>% 
  group_by(condition) %>% 
  summarize(mean_similarity = mean(similarity),
            sd_similarity = sd(similarity)) %>% 
  arrange(desc(mean_similarity))

ggboxplot(semantic_similarities, x="condition", y="similarity", color="condition") +
  stat_summary(fun="mean") 
```

The low-agency condition had the greatest semantic similarity among ideas, followed by the high-agency condition, and then Control condition.

## Crowdsourced evaluations of creativity

The next step will be to have another set of crowdworkers evaluate the creativity of the high-agency participants' ideas in terms of overall creativity, originality, and usefulness.

```{r}

# Prepare idea submissions dataset for evaluation by another group of crowdworkers
# The data needs to be in long format, where each row represents a single idea for a single object

# In case you want to view wide
idea_submissions_wide <- trial_data %>% 
  mutate(trial = str_extract(trial, "([A-Za-z0-9]+)(_)([A-Za-z0-9]+)$")) %>% 
  pivot_wider(id_cols = c("ResponseId", "condition", "process_open-ended"),
              names_from = "trial",
              values_from = "use")

idea_submissions_long <- trial_data %>% 
  unite(col = "idea_id", ResponseId, trial, sep="-", remove = FALSE)
  

write_csv(idea_submissions_long, "data/idea_submissions.csv")

# Create separate csvs for frisbee, brick, and bubblewrap
frisbee_submissions <- idea_submissions_long %>% 
  filter(object=="frisbee")

brick_submissions <- idea_submissions_long %>% 
  filter(object=="brick")

bubble_submissions <- idea_submissions_long %>% 
  filter(object=="bubblewrap")

# Save to csvs

write_csv(frisbee_submissions, "data/frisbee_submissions.csv")
write_csv(brick_submissions, "data/brick_submissions.csv")
write_csv(bubble_submissions, "data/bubble_submissions.csv")
           
```

### Analysis of crowdworkers' subjective evaluations

N = 99 participants

```{r}
# Read in the crowdworker evaluations
# Each row represents a crowdworkers' responses. The data is super wide because each column represents a response to a single high-agency study participant's idea
# First, wrangle the data so that the column format is similar to the high-agency study data: frisbee_1, frisbee_2...bubble_5

crowdworkers <- read_csv("data/crowdworker_eval_2025.09.18.csv")

# Check which idea submitter is tied to which crowdworker
submitter_ids <- crowdworkers %>% 
  slice_head(n=1) %>% 
  pivot_longer(cols = 14:2038, 
               names_to = "participant_trial",
               values_to = "submitter_id") %>% 
  select(participant_trial, submitter_id) %>% 
  mutate(submitter_id = str_extract(submitter_id, "R_[A-Za-z0-9]+")) %>% 
  separate(participant_trial,
           into = c("person_id", "trial"),
           extra = "merge") %>% 
    separate(col = trial,
           into = c("metric", "trial"),
           sep = "_",
           extra = "merge") %>% 
  mutate(metric = case_when(metric == "c" ~ "creativity",
                            metric == "o" ~ "originality",
                            metric == "u" ~ "usefulness")) %>% 
  distinct(person_id, submitter_id)


crowdworker_eval <- crowdworkers %>% 
  filter(DistributionChannel=="anonymous") 

crowdworker_passed_attn_check <- crowdworker_eval %>% 
  filter(attn_check == "4" & attn_check_4_TEXT == "8")


```

```{r}
# Ensure there is a column for the ID of the high-agency participant that the submitted idea came from
crowdworker_eval_long <- crowdworker_passed_attn_check %>% 
  pivot_longer(cols = 14:2038,
               names_to = "participant_trial",
               values_to = "rating") %>% 
  drop_na(rating) %>% 
  separate(col = participant_trial, 
           into = c("person_id", "trial"),
           sep = "_",
           extra = "merge") %>% 
  mutate(rating = as.numeric(rating)) %>% 
  separate(col = trial,
           into = c("metric", "trial"),
           sep = "_",
           extra = "merge") %>% 
  mutate(metric = case_when(metric == "c" ~ "creativity",
                            metric == "o" ~ "originality",
                            metric == "u" ~ "usefulness")) %>% 
  unite(col="participant_rating_id", ResponseId, metric, trial, remove = FALSE) %>% 
  mutate(trial_id = str_remove(participant_rating_id, "_creativity|_originality|_usefulness")) %>% 
  pivot_wider(id_cols=c(trial_id, person_id), 
              names_from = "metric",
              values_from = "rating")


crowdworkers_submitters <- left_join(crowdworker_eval_long, submitter_ids, by = "person_id") %>% 
  extract(trial_id, 
          into = c("crowdworker_id", "trial"),
          regex = "(R_[A-Za-z0-9]+)_(.*)")




```

```{r}
# Merge with trial data from the high-agency
# participantID refers to the CloudResearch ID of the idea submitter

trial_data_renamed <- trial_data %>% 
  rename(submitter_id = "ResponseId") %>% 
  mutate(trial = str_remove(trial, "^[^_]*_")) %>% 
  select(submitter_id, condition, trial, object, use, participantId)

crowdworkers_submitters_ideas <- full_join(crowdworkers_submitters, trial_data_renamed, by=c("submitter_id", "trial"))

# Descriptives of overall creativity, originality, and usefulness by condition
crowdworkers_submitters_ideas %>% 
  group_by(condition) %>% 
  get_summary_stats(c(creativity, originality, usefulness),
                    type="mean_sd") %>% 
  arrange(variable)


```

### Compensate bonuses for those whose ideas were rated as greater than 3 out of 5 on the creativity scales

7 out of the 45 high-agency study participants met the criteria for a bonus.

```{r}
bonuses <- crowdworkers_submitters_ideas %>% 
  group_by(participantId) %>% 
  get_summary_stats(c(creativity, originality, usefulness),
                    type="mean_sd") %>% 
  pivot_wider(id_cols = "participantId",
              names_from = "variable",
              values_from = "mean") %>% 
  filter(originality > 3 & usefulness > 3) %>% 
  select(-c(creativity, originality, usefulness)) %>% 
  rename("Participant or Assignment" = participantId) %>% 
  mutate(Amount = 1.00,
         Message = "Great job! Your ideas scored high on creativity, so you receive a bonus.") 

write_csv(bonuses, "data/high-agency_bonus.csv")
  

```

```{r}
crowdworkers_submitters_ideas %>% 
  pivot_longer(cols=c(creativity, originality, usefulness),
               names_to = "metric",
               values_to = "rating") %>% 
  ggplot(aes(x=condition, y=rating)) +
  geom_boxplot() +
  stat_summary(fun="mean") +
  facet_wrap(~metric)

```

```{r}
# Distributions of ratings for each condition
crowdworkers_submitters_ideas %>% 
  pivot_longer(cols=c(creativity, originality, usefulness),
               names_to = "metric",
               values_to = "rating") %>% 
  ggplot(aes(x=rating)) +
  geom_histogram(bins=5,
                 color="white",
                 aes(y=after_stat(density))) +
  facet_wrap(~metric+condition)

```

```{r}
# For each pair of submitter_id and crowdworker_id we can compute mean ratings
crowdworkers_submitters_ideas %>% 
  group_by(submitter_id, crowdworker_id) %>% 
  summarize(mean_creativity = mean(creativity),
         mean_originality = mean(originality),
         mean_usefulness = mean(usefulness)) %>% 
  group_by(submitter_id) %>% 
  mutate(rater_num = row_number()) %>% 
  ungroup() 



# What is the relationship between originality and overall creativity? Usefulness and overall creativity? Interaction between the two?


# Comparison of automated vs. manual originality evaluations
manual_ratings <- crowdworkers_submitters_ideas %>% 
  group_by(submitter_id) %>% 
  summarize(mean_creativity = mean(creativity),
         mean_originality = mean(originality),
         mean_usefulness = mean(usefulness)) %>% 
  rename(manual_originality = "mean_originality")

automated_ratings <- originality_trials %>% 
  group_by(ResponseId) %>% 
  summarize(originality = mean(originality)) %>% 
  rename(submitter_id = "ResponseId",
         automated_originality = "originality") 

cor.test(manual_ratings$manual_originality, automated_ratings$automated_originality)

both_ratings <- full_join(manual_ratings, automated_ratings, by="submitter_id")

ggplot(both_ratings, aes(x=manual_originality, y=automated_originality)) +
  geom_point()


```

## Similarity Model

```{r}
# originality_mod <- lm(originality ~ )
# 
# kable(tidy(mod), digits=2)
# 
# tstats <- t.test()

semantic_similarity_mod <- lmer(similarity ~ condition + (1|object) + (1|ResponseId), data=semantic_similarities)
summary(semantic_similarity_mod)
```

Examine differences (`r apa_print(tstats)$statistic`).

Template to render in APA style can be found here: https://wjschne.github.io/apaquarto/installation.html

# Exploratory Next Steps

## Chat log data: Actual interactions with AI

We want to compare the interaction behaviors of low-agencys and high-agencys. We expect that low-agencys will copy-paste more than high-agencys. Generally, their submitted ideas should look more like what ChatGPT suggested. We can also look at the prompting strategies of low-agencys vs. high-agencys.

```{r}
chat_log <- read_csv("data/cleaned_user_ai_conversations.csv")
```

```{r}


```

## Process data as a moderator of creative outcomes

We asked participants to report the extent to which they (1) felt they were engaging in dialogue with AI, (2) felt that AI helped augment their creativity, and (3) found the task intrinsically motivating. We might expect, for example, that high-agencys produce more diverse ideas *only if* they perceive that AI helped them consider unexpected angles of the object.

Regardless of experimental condition, we might find systematic differences in AI usage when we compare the most vs. least creative participants. How can we characterize those differences?

# Power Analysis

I tried a simple power analysis in GPower, but that wasn't the best for my data and study design. So now I'm using a simulation:

```{r, echo=FALSE}
# Create a mixed model from high-agency data
manual_ratings_model <- lmer(
  originality ~ condition + (1|object) + (1|submitter_id), data=crowdworkers_submitters_ideas)
summary(manual_ratings_model)

# # Decide how many submitters
unique_submitters <- unique(crowdworkers_submitters_ideas$submitter_id)
#   
# # Decide how many raters needed to evaluate those 
# unique_raters <- unique(crowdworkers_submitters_ideas$crowdworker_id)
# unique_raters_sample <- sample(unique_raters, size = S, replace = TRUE)

# Sample submitters
# Get all of their ideas
# Rename their ideas so they're unique
# Combine into simulation dataset
# Repeat until you have the number of ratings specified in N

# Determine total number of ratings
# Then figure out how those ratings ought to be sourced. How many submitters does that require, holding the number of ideas and objects constant.
# Account for presentation order after the fact, rather than constraining the study design.

# Loop through the sample of submitters
sims <- 1000
S <- 130 # This can be adjusted to test different numbers of submitters

originality_summaries <- list()
for (i in 1:sims) {
  # data_resamp <- crowdworkers_submitters_ideas %>%
  # slice_sample(n = N, replace = TRUE)
  
  unique_submitters_sample <- sample(unique_submitters, size = S, replace = TRUE)
  
  data_sample_list <- list()
  for (j in 1:length(unique_submitters_sample)){
    data_sample_list[[j]] <- crowdworkers_submitters_ideas %>% 
      filter(submitter_id == unique_submitters_sample[j]) %>% 
      mutate(submitter_id = paste0(submitter_id, j)) 
    
  }
  data_resamp <- do.call(rbind, data_sample_list)
  manual_ratings_model <- lmer(
  originality ~ condition + (1|object) + (1|submitter_id), data=data_resamp)
  originality_summaries[[i]] <- summary(manual_ratings_model)$coefficients[2:3, 4] # Extract t-values, which are in the fourth column
}

originality_summaries_bound <- do.call(rbind, originality_summaries)

# t must be greater than 2 to be significant. We just need absolute value

power_low-agency <- mean(abs(originality_summaries_bound[,1]) > 2)
power_high-agency <- mean(abs(originality_summaries_bound[,2]) > 2)

power_low-agency
power_high-agency
  

```

```{r, echo=FALSE}
# # Create simulated model using extracted parameters
# model <- makeLmer(originality ~ condition + (1|trial), fixef=fixed, VarCorr=rand, sigma=s, data=originality_trials)
# model
# 
# # Extend the model along ResponseId
# model_ext <- extend(model, along="trial", n=500)
# model_ext
```

## IRiSS Sample

```{r}
iriss <- read_csv("data/iriss.csv") %>% 
  slice(-(1:2)) %>% 
  filter(attn_check == "4" & attn_check_4_TEXT == "8") %>% 
  rename(condition = FL_3_DO) %>% 
  relocate(condition, .after = attn_check_4_TEXT) %>% 
  mutate(condition = case_when(condition == "GuidedInstructions" ~ "high-agency",
                               condition == "UnguidedInstructions" ~ "low-agency",
                               condition == "ControlInstructions" ~ "Control")) %>% 
  filter(Progress=="100") %>% 
  rename(submitter_id = "ResponseId") %>% 
  mutate(age = as.numeric(age))

iriss %>% 
  get_summary_stats(age, type="mean_sd")


```

```{r}
submitter_demographics <- iriss %>%
  mutate(gender = case_when(gender == "1" ~ "Male",
                            gender == "2" ~ "Female",
                            gender == "3" ~ "Non-binary",
                            gender == "4" ~ "Prefer to self-describe")) %>% 
  mutate(race = case_when(race == "1" ~ "Black or African-American",
                          race == "2" ~ "American Indian or Alaskan Native",
                          race == "3" ~ "White or Caucasian",
                          race == "4" ~ "Native Hawaiian",
                          race == "5" ~ "Hispanic, Latino or Spanish Origin",
                          race == "6" ~ "Asian-American",
                          race == "7" ~ "Multiracial or self-described")) %>% 
  mutate(race = fct_infreq(race)) %>% 
  dplyr::select(submitter_id, age, gender, race) 

submitter_demographics %>% 
  tbl_summary(include = c(age, gender, race),
              statistic = list(
                all_continuous() ~ "{mean} ({sd})",
                all_categorical() ~ "{n} / {N} ({p}%)"),
              digits = all_continuous() ~ 1,
              label = list(age = "Age",
                           gender = "Gender",
                           race = "Race")) %>% 
   modify_header(label ~ "**Demographic**")


```

```{r}

iriss_memory_check <- iriss %>% 
  mutate(memory_check_condition = case_when(memory_check == "1" ~ "high-agency",
                                            memory_check == "2" ~ "low-agency",
                                            is.na(memory_check) ~ "Control")) %>% 
  relocate(memory_check_condition, .after = "memory_check") %>% 
  mutate(memory_check_result = if_else(condition == memory_check_condition, 1, 0)) %>% 
  relocate(memory_check_result, .after = "memory_check_condition")

# Check the % of participants who remembered what the instructions said
iriss_memory_check %>% 
  count(memory_check_result) %>% 
  mutate(prop=n/sum(n))

iriss_memory_check %>% 
  count(condition)


```

```{r}
# Convert to long format
iriss_trial_data <- iriss %>% 
  pivot_longer(names_to = "trial",
                values_to = "use",
                cols = starts_with(c("pilot", "pass", "control"))) %>% 
  mutate(trial = str_remove(trial, "open_")) %>% # fixed naming problem
  filter(grepl("^(pilot|pass|control)_(brick|frisbee|bubble)_[1-5]$", trial)) %>% 
  filter(!is.na(use)) %>% # Keep only the trials with responses
  relocate(trial, .after = "condition") %>% 
  relocate(use, .after = "trial") %>% 
  mutate(object = str_extract(trial, "(?<=_).*")) %>% 
  mutate(object = str_remove(object, "_.*")) %>% 
  relocate(object, .after = "trial") %>% 
  mutate(object = if_else(object == "bubble", "bubblewrap", object))

write_csv(iriss_trial_data, "data/iriss_trial_data.csv")
```

```{r}
iriss_self_perceptions <- iriss %>% 
  pivot_longer(names_to = "metric",
               values_to = "rating",
               cols = c(ends_with("_unique"),
                        matches("self_[1-3]$"))) %>% 
  drop_na(rating) %>% 
  mutate(metric = str_replace_all(metric, 
                                  c("self_1" = "creativity",
                                    "self_2" = "originality",
                                    "self_3" = "usefulness"))) %>% 
  separate(
    col = metric,
    into = c("condition", "object", "question"),
    sep = "_",
    remove = TRUE# keep original column if you still want it
  ) %>% 
  mutate(
    object = str_replace_all(
      object,
      c("\\bfris\\b" = "frisbee",
        "\\bbubb\\b"  = "bubble"))) %>% 
  mutate(condition = case_when(condition == "pilot" ~ "High-Agency",
                               condition == "pass" ~ "Low-Agency",
                               condition == "control" ~ "Control")) %>% 
  pivot_wider(names_from = "question",
              values_from = "rating") %>% 
  mutate(across(c(creativity, originality, usefulness), as.numeric))

```

```{r}
# Self-perceptions of creativity, originality, usefulness

self_perception_outcomes <- c("originality", "creativity", "usefulness")

self_perception_plots <- map(self_perception_outcomes, function(var) {
  summary_df <- iriss_self_perceptions %>%
    mutate(condition = fct_relevel(as.factor(condition), c("Low-Agency", "High-Agency", "Control"))) %>% 
    group_by(submitter_id, condition) %>%
    summarize(value = mean(.data[[var]]), .groups = "drop")
  
  ggplot(summary_df, aes(x = value, y = condition, fill = condition)) +
    geom_density_ridges(alpha = 0.6, quantile_lines = TRUE, quantile_fun = mean) +
    xlim(c(1,5)) +
    labs(x = paste0("Self-perceived ", var), y = "") +
    theme_apa() +
    theme(legend.position = "none")
})

# To display the plots:
self_originality <- self_perception_plots[[1]]  # originality
self_creativity<- self_perception_plots[[2]]  # creativity
self_usefulness <- self_perception_plots[[3]]  # usefulness

self_creativity
self_originality
self_usefulness

# Combined plot
combined_self_plot <- self_creativity + self_originality + self_usefulness + plot_annotation(tag_levels = "a")
combined_self_plot

ggsave(plot=combined_self_plot, filename="figures/self_perceptions.png")
# Mixed model


```

```{r}
# Relationship between self-perception and third-party perception
iriss_self_perceptions %>% 
  rename(self_creativity = creativity,
         self_originality = originality,
         self_usefulness = usefulness) 

# Correlation?
```

```{r}
# How unique did participants think their ideas were?
perceived_uniqueness <- iriss_self_perceptions %>% 
  drop_na(unique) %>% 
  group_by(condition) %>% 
  count(unique) %>% 
  mutate(prop=n/sum(n)) %>% 
  ggplot(aes(x=unique, y=prop, fill=condition)) +
  geom_col(position = "dodge") +
  scale_x_discrete(labels = c(
    "1" = "No one",
    "2" = "Some people",
    "3" = "Most people",
    "4" = "Everyone"
  )) +
  labs(x="How many people came up with this idea?",
       y="Proportion of responses") +
  theme_apa()

ggsave("figures/uniqueness.png", plot=perceived_uniqueness)
```

```{r}
# How does mean originality compare across conditions?

iriss_self_perceptions %>% 
  drop_na(originality) %>% 
  group_by(ResponseId, condition) %>% 
  summarize(originality = mean(originality)) %>% 
  ggplot(aes(x=condition, y=originality)) +
  geom_boxplot() +
  stat_summary(fun="mean") 
  labs(y="Self-perceived originality")
```

### Self perceptions

```{r}
# Make sure the reference group is the Control group!
iriss_self_originality_mod <- lmer(
  originality ~ condition + (1|object) + (1|submitter_id), data= iriss_self_perceptions)
summary(iriss_self_originality_mod)

iriss_self_creativity_mod <- lmer(
  creativity ~ condition + (1|object) + (1|submitter_id), data= iriss_self_perceptions)
summary(iriss_self_creativity_mod)

iriss_self_usefulness_mod <- lmer(
  usefulness ~ condition + (1|object) + (1|submitter_id), data= iriss_self_perceptions)
summary(iriss_self_usefulness_mod)
```

high-agencys perceived their ideas as more useful than the other participants.

```{python}
from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"


# Load the sentence transformer model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Load the data

data_path = '~/Git_Projects/idea_generation/data/iriss_trial_data.csv'  # Update with your actual data path
df = pd.read_csv(data_path)

# For each combination of experimental condition, object, and ResponseId, compute the semantic similarity of ideas
results = []
for (condition, obj, response_id), group in df.groupby(['condition', 'object', 'ResponseId']):
    ideas = group['use'].tolist()

    # Compute embeddings
    embeddings = model.encode(ideas)

    # Compute cosine similarity matrix
    norm_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
    similarity_matrix = np.dot(norm_embeddings, norm_embeddings.T)

    # Extract upper triangle of the similarity matrix, excluding the diagonal
    upper_tri_indices = np.triu_indices_from(similarity_matrix, k=1)
    similarities = similarity_matrix[upper_tri_indices]

    # Store results
    for sim in similarities:
        results.append({
            'condition': condition,
            'object': obj,
            'ResponseId': response_id,
            'similarity': sim
        })

# Convert results to DataFrame
results_df = pd.DataFrame(results)
summary = results_df.groupby('condition')['similarity'].agg(['mean', 'std'])
print(summary)

# Save results to CSV
output_path = os.path.expanduser('~/Git_Projects/idea_generation/data/iriss_semantic_similarities.csv')  # Update with your desired output path
results_df.to_csv(output_path, index=False)
```

## H2: Semantic Similarity

```{r}
iriss_semantic_similarities <- read_csv("data/iriss_semantic_similarities.csv") %>% 
  rename(submitter_id = ResponseId)
iriss_semantic_similarity_mod <- lmer(similarity ~ condition + (1|object) + (1|submitter_id), data=iriss_semantic_similarities)
summary(iriss_semantic_similarity_mod)

sjPlot::tab_model(iriss_semantic_similarity_mod)


```

According to the linear mixed model, low-agencys produce significantly more similar ideas than the Control group.

```{r}
iriss_semantic_similarities %>% 
  group_by(condition, submitter_id) %>% 
  summarize(similarity = mean(similarity)) %>% 
  ggplot(aes(x=condition, y=similarity)) +
  geom_boxplot() +
  stat_summary(fun="mean") 
```

```{r}
emmeans(iriss_semantic_similarity_mod, pairwise ~ condition, adjust="bonferroni")
```

## Crowdsourced originality ratings

```{r}
iriss_manual_ratings <- read_csv("data/iriss_manual_ratings.csv") %>% 
  filter(DistributionChannel=="anonymous") %>% 
  filter(attn_check == "4" & attn_check_4_TEXT == "8") %>% 
  drop_na(frisbee.0.condition)  %>%  # Drop rows with missing data (experimenter error) 
  mutate(age = as.numeric(age))


```

```{r}
evaluator_demographics <- iriss_manual_ratings %>% 
  mutate(age = if_else(age < 18 | age > 100, NA_real_, age)) %>% 
  mutate(gender = case_when(gender == "1" ~ "Male",
                            gender == "2" ~ "Female",
                            gender == "3" ~ "Non-binary",
                            gender == "4" ~ "Prefer to self-describe")) %>% 
  mutate(race = case_when(race == "1" ~ "Black or African-American",
                          race == "2" ~ "American Indian or Alaskan Native",
                          race == "3" ~ "White or Caucasian",
                          race == "4" ~ "Native Hawaiian",
                          race == "5" ~ "Hispanic, Latino or Spanish Origin",
                          race == "6" ~ "Asian-American",
                          race == "7" ~ "Multiracial or self-described")) %>% 
  mutate(race = fct_infreq(race)) %>% 
  select(age, gender, race) %>% 
  tbl_summary(include = c(age, gender, race),
              statistic = list(
                all_continuous() ~ "{mean} ({sd})",
                all_categorical() ~ "{n} / {N} ({p}%)"),
              digits = all_continuous() ~ 1,
              label = list(age = "Age",
                           gender = "Gender",
                           race = "Race")) %>% 
   modify_header(label ~ "**Demographic**")
evaluator_demographics

combined_demographics <- tbl_merge(
  tbls = list(submitter_demographics, evaluator_demographics),
  tab_spanner = c("**Submitters**", "**Evaluators**")
)

combined_demographics
```

```{r}
iriss_manual_ratings_long <- iriss_manual_ratings %>% 
  rename(rater_id = ResponseId) %>% 
  pivot_longer(cols = frisbee.0.condition:bubblewrap.4.use,
               names_to = "stimulus_metadata",
               values_to = "value") %>% 
  mutate(metadata = str_extract(stimulus_metadata, "[^.]+$"),
         stimulus_metadata = str_remove(stimulus_metadata, "\\.[^.]+$")) %>% 
  separate_wider_delim(stimulus_metadata,
           ".",
           names = c("object", "trial_num")) %>% 
  pivot_wider(
              names_from = "metadata",
              values_from = "value") %>% 
  rename(submitter_id = ResponseId)

# Pivot to give each creativity rating its own column
iriss_manual_ratings_metrics <- iriss_manual_ratings_long %>% 
  pivot_longer(cols = c_frisbee_1:u_bubble_5,
               names_to = "trial",
               values_to = "rating") %>% 
  mutate(rating = as.numeric(rating)) %>% 
  mutate(metric = str_extract(trial, "^[A-Za-z]+(?=_)")) %>% 
  mutate(metric = case_when(metric == "c" ~ "creativity",
                            metric == "o" ~ "originality",
                            metric == "u" ~ "usefulness")) %>% 
  dplyr::select(rater_id, object, trial_num, submitter_id, condition, use, trial, metric, rating) %>% 
  mutate(trial = str_remove(trial, "^._")) %>% 
  mutate(trial_num = as.numeric(trial_num),
         trial_num = trial_num+1) %>% 
  unite(col="stimulus", c("object", "trial_num")) %>% 
  mutate(trial = str_replace_all(trial, "bubble", "bubblewrap")) %>% # Check names!
  filter(stimulus==trial) %>% 
  pivot_wider(names_from = "metric",
              values_from = "rating") %>% 
  mutate(object = str_extract(stimulus, "^[A-Za-z]+")) %>% 
  relocate(object, .after = "stimulus") %>% 
  mutate(use_word_count = str_count(use, "\\S+")) %>% 
  relocate(use_word_count, .after = use) %>% 
  mutate(condition=case_when(condition=="Pilot"~"High-Agency",
                             condition=="Passenger"~"Low-Agency",
                             condition=="Control"~"Control"))

# Select process variables from full dataset
process_vars <- iriss_trial_data %>% 
  dplyr::select(submitter_id, object, use, starts_with("conv_with_ai"), starts_with("consideration_aid"), starts_with("intrinsic_motivation"))



write_csv(iriss_manual_ratings_metrics, "data/iriss_manual_ratings_metrics.csv")
```

### Descriptives

```{r}
iriss_manual_ratings_metrics %>% 
  group_by(condition) %>% 
  get_summary_stats(use_word_count, type="mean_sd")

iriss_manual_ratings_metrics %>% 
  ggplot(aes(x=condition, y=use_word_count)) +
  geom_boxplot() +
  stat_summary(fun="mean")

cor.test(iriss_manual_ratings_metrics$use_word_count, iriss_manual_ratings_metrics$originality)

ggplot(iriss_manual_ratings_metrics, aes(x=use_word_count, y=originality, color=condition)) +
  geom_jitter() +
  geom_smooth(method="lm")


```

Higher word count is associated with greater originality.

### Originality analysis

```{r}
iriss_manual_ratings_metrics %>% 
  group_by(condition) %>%
  get_summary_stats(c(creativity, originality, usefulness), type="mean_sd") %>% 
  arrange(variable)

iriss_manual_ratings_metrics %>% 
  group_by(condition) %>% 
  count(originality) %>% 
  mutate(prop=n/sum(n)) %>% 
  ggplot(aes(x=originality, y=prop)) +
  geom_col(color="white") +
  facet_wrap(~condition)

# Descriptives table
summary_table <- iriss_manual_ratings_metrics %>%
  mutate(condition=fct_relevel(as.factor(condition), "low-agency", "high-agency", "Control")) %>% 
  group_by(condition) %>%
  get_summary_stats(c(creativity, originality, usefulness), type = "mean_sd") %>%
  mutate(mean_sd = sprintf("%.2f (%.2f)", mean, sd)) %>%
  select(condition, variable, mean_sd) %>%
  pivot_wider(names_from = variable, values_from = mean_sd)

summary_table %>%
  kbl(
    col.names = c("Condition", "Creativity", "Originality", "Usefulness"),
    align = "lccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    html_font = "helvetica",
    full_width = FALSE,
    position = "center",
  ) 
```

### Heatmap of creativity "sweet spots"

```{r}
label_points <- iriss_manual_ratings_metrics %>% 
  anti_join(noncompliant_conversations, by=c("submitter_id", "object")) %>% 
  anti_join(unevaluated_conversations, by=c("submitter_id", "object")) %>% 
  group_by(condition) %>% 
  sample_n(5)

# Might want to change the colors 
iriss_manual_ratings_metrics %>% 
  anti_join(noncompliant_conversations, by=c("submitter_id", "object")) %>% 
  anti_join(unevaluated_conversations, by=c("submitter_id", "object")) %>% 
  group_by(originality, usefulness) %>% 
  summarize(creativity=mean(creativity)) %>% 
  ggplot(aes(x=originality, y=usefulness, fill=creativity)) +
  geom_tile(color="black") +
  geom_label_repel(
    data=label_points,
    aes(label = str_wrap(paste(object, use, sep = ": "), width = 20)),
    size=2.5,
    color="black",
    alpha=.75) +
  facet_wrap(~condition,
             labeller = as_labeller(c(
      "Control" = "Control",
      "Passenger" = "Low-Agency",
      "Pilot" = "High-Agency"
    ))) +
  scale_fill_gradient(low = "darkgrey", 
                      high = "gold",
                      breaks  = range(iriss_manual_ratings_metrics$creativity, na.rm = TRUE), 
                      labels = c("Not at all", "Maximally")) +
  labs(fill="Evaluators'\n creativity rating",
       x="Originality",
       y="Usefulness") +
  theme_apa() +
  theme(
    legend.position = "bottom",
    legend.title = element_text(vjust = 1, hjust = 0.5),
    legend.text.align = 0.5
  ) +
  guides(
    fill = guide_colorbar(
      title.position = "top",    # put title above bar
      title.hjust = 0.5,         # center title
      label.hjust = 0.5,         # center tick labels
      barwidth = unit(6, "cm"),  # adjust width as needed
      barheight = unit(0.4, "cm") 
    )
  )
```

```{r}
iriss_manual_ratings_metrics %>% 
  group_by(submitter_id, condition) %>% 
  summarize(originality=mean(originality)) %>%
  ggplot(aes(x=originality, y=condition)) +
  geom_density_ridges()
```

### Plots for H1

```{r}
outcomes <- c("originality", "creativity", "usefulness")

plots <- map(outcomes, function(var) {
  summary_df <- iriss_manual_ratings_metrics %>%
    group_by(submitter_id, condition) %>%
    summarize(value = mean(.data[[var]]), .groups = "drop")
  
  ggplot(summary_df, aes(x = value, y = condition, fill = condition)) +
    geom_density_ridges(alpha=.6, quantile_lines = TRUE, quantile_fun = mean) +
    xlim(c(1,5)) +
    labs(x = var, y = "") +
    theme_apa() +
    theme(legend.position = "none")
})

# To display the plots:
originality <- plots[[1]]  # originality
creativity <- plots[[2]]  # creativity
usefulness <- plots[[3]]  # usefulness

ggsave("figures/originality.png", plot = plots[[1]])
ggsave("figures/creativity.png", plot = plots[[2]])
ggsave("figures/usefulness.png", plot = plots[[3]])


# Combined plot
combined_evaluation_plot <- creativity + originality + usefulness + plot_annotation(tag_levels = "a")
combined_evaluation_plot

ggsave(plot=combined_evaluation_plot, filename="figures/evaluations.png")
```

```{r}
semantic_similarity <- ggplot(iriss_semantic_similarities, aes(x=similarity, y=condition, fill=condition)) +
  geom_density_ridges(alpha=.6,
                      quantile_lines = TRUE,
                      quantile_fun = mean) +
  scale_x_continuous(breaks=seq(-.5, 1, by=.1)) +
  labs(x="cosine similarity") +
  theme_apa() +
  theme(legend.position = "none") 

ggsave("figures/semantic_similarity.png", plot=semantic_similarity)
```

## H1

```{r}
iriss_originality_mod <- lmer(
  originality ~ condition + (1|object) + (1|submitter_id), data= iriss_manual_ratings_metrics)
summary(iriss_originality_mod)
anova(iriss_originality_mod)

coef(summary(iriss_originality_mod))

# anova(iriss_originality_mod)
# F_to_eta2(4.45, df=2, df_error=120)
# eta_squared(compliant_semantic_similarity_mod)


emmeans(iriss_originality_mod, pairwise ~ condition, adjust = "bonferroni")

# With demographic covariates
demographics <- iriss %>% 
  mutate(gender = case_when(gender == "1" ~ "Male",
                            gender == "2" ~ "Female",
                            gender == "3" ~ "Non-binary",
                            gender == "4" ~ "Prefer to self-describe")) %>% 
  mutate(race_recoded = if_else(race == "3", "White", "Non-White")) %>%
  select(submitter_id, age, race_recoded, gender, education)

iriss_manual_ratings_demographics <- left_join(iriss_manual_ratings_metrics, demographics, by="submitter_id")

#Adjust
iriss_originality_demographics <- lmer(
  originality ~ condition + scale(age) + as.factor(gender) + as.factor(race_recoded) + as.factor(education) + (1|object) + (1|submitter_id), data= iriss_manual_ratings_demographics)
summary(iriss_originality_demographics)
coef(summary(iriss_originality_demographics))


# Repeat models for creativity and usefulness
iriss_creativity_mod <- lmer(
  creativity ~ condition + (1|object) + (1|submitter_id), data= iriss_manual_ratings_metrics)
summary(iriss_creativity_mod)

iriss_usefulness_mod <- lmer(
  usefulness ~ condition + (1|object) + (1|submitter_id), data= iriss_manual_ratings_metrics)
summary(iriss_usefulness_mod)
```

Which conditions contributed the most creative ideas?

```{r}
iriss_manual_ratings_metrics %>% 
  filter(originality > 3 & usefulness > 3) %>% 
  count(condition) 
```

low-agency participants produced the most creative ideas, followed by high-agency participants, then participants in the control condition.


What were the highest-performing ideas?
```{r}
iriss_manual_ratings_metrics %>% 
  arrange(desc(creativity)) %>% 
  dplyr::select(object, submitter_id, condition, use, creativity, originality, usefulness) %>% 
  print(n=100)
```


### Most common uses for each object

```{r}
iriss_manual_ratings_metrics %>% 
  mutate(use = str_to_lower(use)) %>% 
  group_by(object) %>% 
  count(use) %>% 
  arrange(desc(n)) %>% 
  print(n=30)
```

### Noncompliance in AI Access conditions

This is JUST on the submission side. Here, we identify the trials on which high-agencys and low-agencys did not use AI, regardless of whether ideas from those conversations got evaluated.
```{r}
no_chats_high_agency <- read_csv("data/pilot_no_first_message.csv")

no_chats_low_agency <- read_csv("data/passenger_no_first_message.csv")

noncompliant_conversations <- bind_rows(no_chats_high_agency, no_chats_low_agency) %>% 
  rename(submitter_id = participant_id)


# Number of noncompliant conversations
noncompliant_conversations %>% 
  group_by(condition) %>% 
  summarize(n_distinct(submitter_id))
```

## Subsetting of conversations and participants based on which conversations were evaluated and which were compliant
```{r}
# Number of conversations that actually got rated
iriss_conversations <- iriss_trial_data %>% 
  group_by(submitter_id, condition, object) %>%
  summarize(aggregated_uses = paste(use, collapse = "; ")) 

compliant_conversations <- iriss_conversations %>% 
  anti_join(noncompliant_conversations)

evaluated_conversations <- iriss_manual_ratings_metrics %>% 
  group_by(submitter_id, condition, object) %>% 
  summarize(aggregated_uses = paste(use, collapse = "; "))

# unevaluated_conversations <- iriss_conversations %>% 
#   anti_join(evaluated_conversations, by=c("submitter_id", "condition", "object"))
```


### Evaluated, compliant conversations

```{r}
# Exclude the noncompliant conversations from the analysis
evaluated_compliant_conversations <- evaluated_conversations %>% 
  anti_join(noncompliant_conversations, by=c("submitter_id", "condition", "object"))

evaluated_compliant_conversations %>%
  group_by(condition) %>%
  summarize(unique_people = n_distinct(submitter_id))


write_csv(evaluated_compliant_conversations, "data/evaluated_compliant_conversations.csv")



iriss_conversations %>% 
  anti_join(noncompliant_conversations, by=c("submitter_id", "object"))
# 
# compliant_trials <- iriss_manual_ratings_metrics %>%
#   anti_join(noncompliant, by=c("submitter_id", "object")) %>%
#   left_join(process_vars, by=c("submitter_id", "object", "use"), relationship="many-to-many") %>%
#   mutate(across(conv_with_ai_1:intrinsic_motivation_4, as.numeric),
#          intrinsic_motivation_mean = rowMeans(
#       across(intrinsic_motivation_1:intrinsic_motivation_4),
#       na.rm = TRUE
#     )) %>%
#     mutate(
#          conv_with_ai_mean = rowMeans(
#       across(conv_with_ai_1:conv_with_ai_3),
#       na.rm = TRUE
#     )) %>%
#     mutate(
#          consideration_aid_mean = rowMeans(
#       across(consideration_aid_1:consideration_aid_3),
#       na.rm = TRUE
#     ))

# compliant_trials %>% 
#   distinct(condition, submitter_id, object) %>% 
#   filter(condition != "Control") %>% 
#   count(condition)
# 
# compliant_trials %>% 
#   distinct(condition, submitter_id) %>% 
#   count(condition)

# Which participants were completely excluded because all of their trials were noncompliant?


# excluded_participants <- noncompliant %>%
#   count(submitter_id, condition) %>%
#   filter(n==3) %>% 
#   arrange(condition)
# 
# write_csv(excluded_participants, "data/fully_excluded_participants.csv")
# 
# excluded_participants %>% 
#   count(condition)
# 
# # Which participants had AT LEAST ONE noncompliant trial?
# flagged_participants <- noncompliant %>%
#   distinct(submitter_id, condition)


evaluated_compliant_ideas <- evaluated_compliant_conversations %>% 
  separate_rows(aggregated_uses, sep = "; ") %>% 
  rename(use="aggregated_uses") %>% 
  distinct(submitter_id, condition, object, use)
```

### Evaluated compliant ideas

```{r}
write_csv(evaluated_compliant_ideas, "data/evaluated_compliant_ideas.csv")
```

```{r}
evaluated_compliant_ideas %>% 
  mutate(use = str_to_lower(use)) %>% 
  group_by(condition, object) %>% 
  count(use) %>% 
  arrange(desc(n)) %>% 
  print(n=1500)
```




```{python}
from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"


# Load the sentence transformer model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Load the data

data_path = '~/Git_Projects/idea_generation/data/evaluated_compliant_ideas.csv'  # Update with your actual data path
df = pd.read_csv(data_path)

# For each combination of experimental condition, object, and ResponseId, compute the semantic similarity of ideas
results = []
for (condition, obj, response_id), group in df.groupby(['condition', 'object', 'submitter_id']):
    ideas = group['use'].tolist()

    # Compute embeddings
    embeddings = model.encode(ideas)

    # Compute cosine similarity matrix
    norm_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
    similarity_matrix = np.dot(norm_embeddings, norm_embeddings.T)

    # Extract upper triangle of the similarity matrix, excluding the diagonal
    upper_tri_indices = np.triu_indices_from(similarity_matrix, k=1)
    similarities = similarity_matrix[upper_tri_indices]

    # Store results
    for sim in similarities:
        results.append({
            'condition': condition,
            'object': obj,
            'submitter_id': response_id,
            'similarity': sim
        })

# Convert results to DataFrame
compliant_similarity_df = pd.DataFrame(results)
summary_output_path = os.path.expanduser('~/Git_Projects/idea_generation/data/compliant_semantic_similarity_summary.csv')
```

## H1 and 2: Compliant analyses


### Age and gender covariates

```{r}
# Add age and gender covariates
demographics_recoded <- submitter_demographics %>% 
  mutate(gender=if_else(gender=="Female",1, 0),
         race=if_else(race=="White or Caucasian",1,0))


similarity_demographics <- left_join(compliant_similarities, demographics_recoded, by="submitter_id") 



```

### Top-performing ideas by condition

```{r}

iriss_manual_ratings_metrics %>% 
  anti_join(noncompliant_conversations) %>% 
  group_by(condition) %>% 
  arrange(    
    desc(creativity),
    desc(originality),
    desc(usefulness)) %>% 
  dplyr::select(object, submitter_id, condition, use, creativity, originality, usefulness) %>% 
  print(n=100)
```

```

### Linear mixed model with compliant data
```{r}

compliant_trials_demographics <- left_join(compliant_trials, demographics_recoded, by="submitter_id")

compliant_trials_demographics %>% 
  group_by(condition) %>% 
  get_summary_stats(usefulness, type="mean_sd")

compliant_originality_mod <- lmer(
  originality ~ condition + scale(age) + as.factor(gender) + as.factor(race) + (1|object) + (1|submitter_id), data=compliant_trials_demographics)

compliant_creativity_mod <- lmer(
  creativity ~ condition + scale(age) + as.factor(gender) + as.factor(race) + (1|object) + (1|submitter_id), data=compliant_trials_demographics)

compliant_usefulness_mod <- lmer(
  usefulness ~ condition + scale(age) + as.factor(gender) + as.factor(race) + (1|object) + (1|submitter_id), data= compliant_trials_demographics
)
summary(compliant_usefulness_mod)
anova(compliant_usefulness_mod, type=3)
emmeans(compliant_usefulness_mod, pairwise ~ condition)


sjPlot::tab_model(compliant_creativity_mod, compliant_originality_mod, compliant_usefulness_mod,
                  pred.labels = c("Intercept",
                                  "Low-Agency Condition",
                                  "High-Agency Condition",
                                  "Age",
                                  "Gender (Female)",
                                  "Race (White)"),
                  string.ci = "95% CI",
                  dv.labels = c("Creativity", "Originality", "Usefulness"))

summary(compliant_originality_mod)   
anova(compliant_originality_mod)
F_to_eta2(0.36, df=2, df_error=124.38)
```

```{r}
evaluated_compliant_ideas %>% 
  group_by(condition) %>% 
  arrange(desc(n))
```

```{r}
compliant_semantic_similarity_mod <- lmer(similarity ~ condition + (1|object) + (1|submitter_id), data=similarity_demographics)
anova(compliant_semantic_similarity_mod)
eta_squared(compliant_semantic_similarity_mod)

compliant_similarity_demographics_mod <- lmer(
  similarity ~ condition + scale(age) + as.factor(gender) + as.factor(race) + (1|object) + (1|submitter_id), data=similarity_demographics)
summary(compliant_similarity_demographics_mod)

sjPlot::tab_model(compliant_similarity_demographics_mod,
                  pred.labels = c("Intercept",
                                  "Low-Agency Condition",
                                  "High-Agency Condition",
                                  "Age",
                                  "Gender (Female)",
                                  "Race (White)"),
                  string.ci = "95% CI",
                  dv.labels = "Homogeneity")

```


```{r}



compliant_similarities <- iriss_semantic_similarities %>% 
  anti_join(noncompliant_conversations, by=c("submitter_id", "object")) %>% 
  mutate(condition=case_when(condition=="Pilot"~"High-Agency",
                             condition=="Passenger"~"Low-Agency",
                             condition=="Control"~"Control"))



```


```{r}
similarity_stats <- aov(similarity ~ condition, data=compliant_similarities)
anova(compliant_semantic_similarity_mod)
F_to_eta2(4.45, df=2, df_error=120.12)
eta_squared(compliant_semantic_similarity_mod)

# Get estimated marginal means
emm <- emmeans(compliant_semantic_similarity_mod, ~ condition, lmerTest.limit=3380)

# With adjustment for multiple comparisons (Tukey is default)
pairs(emm, adjust = "tukey", reverse = TRUE)


```

## Compliant Originality Plot
```{r}

outcomes <- c("originality", "creativity", "usefulness")

plots <- map(outcomes, function(var) {
  summary_df <- compliant_trials_demographics %>%
    anti_join(unevaluated_conversations, by="submitter_id", "object") %>% 
    mutate(condition=case_when(condition=="Pilot"~"High-Agency",
                          condition=="Passenger"~"Low-Agency",
condition=="Control"~"Control")) %>% 
    mutate(condition= fct_relevel(condition, c("Low-Agency", "High-Agency", "Control"))) %>% 
    group_by(submitter_id, condition) %>%
    summarize(value = mean(.data[[var]]), .groups = "drop")
  
  ggplot(summary_df, aes(x = value, y = condition, fill = condition)) +
    geom_density_ridges(alpha = 0.6, quantile_lines = TRUE, quantile_fun = mean) +
    xlim(c(1,5)) +
    labs(x = str_to_title(var), y = "") +
    theme_apa() +
    theme(legend.position = "none")
})

# To display the plots:
originality <- plots[[1]]  # originality
creativity <- plots[[2]]  # creativity
usefulness <- plots[[3]]  # usefulness

ggsave("figures/originality.png", plot = plots[[1]])
ggsave("figures/creativity.png", plot = plots[[2]])
ggsave("figures/usefulness.png", plot = plots[[3]])


# Combined plot
compliant_evaluation_plot <- creativity + originality + usefulness + plot_annotation(tag_levels = "a")
compliant_evaluation_plot

ggsave(plot=compliant_evaluation_plot, filename="figures/compliant_evaluations.png")
```

```{r}
plots <- map2(outcomes, seq_along(outcomes), function(var, idx) {
  summary_df <- compliant_trials_demographics %>%
    anti_join(unevaluated_conversations, by = c("submitter_id", "object")) %>% 
    mutate(condition = case_when(
      condition == "Pilot" ~ "High-Agency",
      condition == "Passenger" ~ "Low-Agency",
      condition == "Control" ~ "Control"
    )) %>% 
    mutate(condition = fct_relevel(condition, c("Low-Agency", "High-Agency", "Control"))) %>% 
    group_by(submitter_id, condition) %>%
    summarize(value = mean(.data[[var]]), .groups = "drop")
  
  p <- ggplot(summary_df, aes(x = value, y = condition, fill = condition)) +
    geom_density_ridges(alpha = 0.6, quantile_lines = TRUE, quantile_fun = mean) +
    xlim(c(1, 5)) +
    labs(x = str_to_title(var), y = "") +
    theme_apa() +
    theme(legend.position = "none")
  
  # Only show y-axis text for the creativity plot (second in outcomes)
  if (var != "creativity") {
    p <- p + theme(axis.text.y = element_blank(),
                   axis.ticks.y = element_blank())
  }
  
  p
})

# Assign individual plots
originality <- plots[[1]]
creativity <- plots[[2]]
usefulness <- plots[[3]]

# Combine plots
compliant_evaluation_plot <- creativity + originality + usefulness + plot_annotation(tag_levels = "a")
compliant_evaluation_plot

# Save combined plot
ggsave(plot = compliant_evaluation_plot, filename = "figures/compliant_evaluations.png", width = 12, height = 4)

```


## Compliant Similarity Plot
```{r}

similarity_means_df <- compliant_similarities %>%
  group_by(condition) %>%
  summarise(mean_similarity = mean(similarity, na.rm = TRUE))

compliant_semantic_similarity_plot <- compliant_similarities %>% 
  mutate(condition= fct_relevel(condition, c("Low-Agency", "High-Agency", "Control"))) %>% 
  ggplot(aes(x=similarity, y=condition, fill=condition)) +
  geom_density_ridges(alpha=.6,
                      quantile_lines = TRUE,
                      quantile_fun = mean) +
  geom_text(
    data = similarity_means_df,
    aes(x = mean_similarity, y = condition, 
        label = round(mean_similarity, 2)),   # label = numeric mean
    nudge_x = -0.05,    # shift to the right of the mean line
    nudge_y = 0.8,  
    size = 3.5,
    color = "black"
  ) +
  scale_x_continuous(breaks=seq(-.5, 1, by=.1)) +
  labs(x="Homogeneity",
       y="") +
  theme_apa() +
  theme(legend.position = "none") 

compliant_semantic_similarity_plot

ggsave("figures/compliant_semantic_similarity.png", plot=compliant_semantic_similarity_plot)
```

Around 24% of the low-agency conversations did not use AI and around 38% of the high-agency conversations did not use AI. Excluding the noncompliant trials did not change the results.

```{r}
similarities_means_plot <- compliant_similarities %>% 
  mutate(condition= fct_relevel(condition, c("Low-Agency", "High-Agency", "Control"))) %>% 
  group_by(condition) %>% 
  get_summary_stats(similarity, type="mean_se") %>% 
  ggplot(aes(x=condition, y=mean, color=condition)) +
  geom_point() +
  geom_errorbar(aes(ymin = mean-se, ymax = mean+se))+
  ylim(c(0,1))
```


```{r}
# Table of descriptives
similarities_descriptives <- compliant_similarities %>% 
  group_by(condition) %>% 
  get_summary_stats(similarity, type="mean_sd") %>% 
  select(-c(variable, n)) %>% 
  gt() %>% 
  fmt_number(
    columns = c(mean, sd),
    decimals = 2
  ) %>% 
  cols_label(
    condition = "Condition",
    mean = "Mean",
    sd = "SD"
  ) %>% 
  tab_header(
    title = "Similarity Scores by Condition"
  ) %>% 
  tab_options(
    table.border.top.color = "white",
    table.border.bottom.color = "white"
  ) %>% 
  opt_table_font(stack = "geometric-humanist") %>% 
  tab_style(
  style = list(
    cell_text(weight = "bold")
  ),
  locations = cells_column_labels()
)

similarities_descriptives





```


Who were the highest-performing participants?
```{r}
iriss_manual_ratings_metrics %>% 
  anti_join(noncompliant_conversations, by=c("submitter_id", "object")) %>% 
  group_by(submitter_id) %>% 
  summarize(mean_creativity = mean(creativity)) %>% 
  arrange(desc(mean_creativity))


most_creative_participants <- iriss_manual_ratings_metrics %>% 
  filter(submitter_id %in% c("R_626PnHWHD8OebPr",
                             "R_665Usq3Hx2qiS2y",
                             "R_1C4pbfUTsEaLjCV",
                             "R_5ebqE9oR2LkKvO9",
                             "R_7wcOe7LsUrh7hzH",
                             "R_5z92Djt5TP9CJBb",
                             "R_7nZ3ULt8wE34mW6",
                             "R_6dB0PTZKZgdwBAS"))
  
```


```{r}
# Factor analysis of intrinsic motivation items

intrinsic_motivation <- ' f =~ intrinsic_motivation_1 + intrinsic_motivation_2 + intrinsic_motivation_3 + intrinsic_motivation_4' 

onefac4items <- cfa(intrinsic_motivation, data=compliant_trials)
summary(onefac4items, fit.measures=TRUE, standardized=TRUE)
```

CFI indicates good fit (.92)

### Intrinsic motivation during task

```{r}
# make a composite intrinsic motivation item
compliant_trials %>% 
  anti_join(unevaluated_conversations, by=c("submitter_id", "object")) %>% 
  group_by(condition) %>% 
  get_summary_stats(intrinsic_motivation_mean, type="mean_sd")

# Plot intrinsic motivation
intrinsic_motivation_p <- compliant_trials %>% 
  distinct(submitter_id, condition, intrinsic_motivation_mean) %>%
  ggplot(aes(x=intrinsic_motivation_mean, y=fct_relevel(condition, "low-agency", "high-agency", "Control"), fill=condition)) +
  geom_density_ridges(
    quantile_lines=TRUE, 
    quantile_fun=mean) +
  scale_fill_manual(
    values = c(
      "Control" = "#619CFF",
      "low-agency" = "#F8766D",
      "high-agency" = "#00BA38"
    )) +
  labs(x="Intrinsic motivation",
       y="Condition") +
  scale_x_continuous(breaks=seq(1, 7, by=1)) +
  coord_cartesian(xlim = c(1,7)) +
  theme_apa() +
  theme(legend.position="none")

ggsave("figures/intrinsic_motivation.png", plot=intrinsic_motivation_p)
```

```{r}
# Statistical test

compliant_submitters <- compliant_trials %>% 
  distinct(submitter_id, condition, conv_with_ai_mean, consideration_aid_mean, intrinsic_motivation_mean)

intrinsic_motivation_model <- stats::aov(intrinsic_motivation_mean ~ condition, data=compliant_submitters)
summary(intrinsic_motivation_model)

emmeans(intrinsic_motivation_model, pairwise ~ condition, adjust = "bonferroni")
# Comparing high-agencys and low-agencys on intrinsic motivation, consideration AI, and conversation with AI



t.test(compliant_submitters$conv_with_ai_mean[compliant_submitters$condition == "high-agency"],
       compliant_submitters$conv_with_ai_mean[compliant_submitters$condition == "low-agency"])

t.test(compliant_submitters$consideration_aid_mean[compliant_submitters$condition == "high-agency"],
       compliant_submitters$consideration_aid_mean[compliant_submitters$condition == "low-agency"])
```

### Self-Perception

```{r}
compliant_self_perception <- iriss_self_perceptions %>%
  anti_join(noncompliant_conversations, by=c("submitter_id", "object")) %>% 
  anti_join(unevaluated_conversations,by=c("submitter_id", "object")) %>% 
  left_join(demographics_recoded, by="submitter_id")

compliant_self_creativity <- lmer(creativity~condition+scale(age.y) + as.factor(gender.y) + as.factor(race.y) + (1|object) + (1|submitter_id), data=compliant_self_perception)
summary(compliant_self_creativity)
anova(compliant_self_creativity)  

compliant_self_originality <- lmer(originality~condition+scale(age.y) + as.factor(gender.y) + as.factor(race.y) + (1|object) + (1|submitter_id), data=compliant_self_perception)
summary(compliant_self_originality)
anova(compliant_self_originality)  


compliant_self_usefulness <- lmer(usefulness~condition+scale(age.y) + as.factor(gender.y) + as.factor(race.y) + (1|object) + (1|submitter_id), data=compliant_self_perception)
summary(compliant_self_usefulness)
anova(compliant_self_usefulness)  

```

```{r}
# For each idea that the participant self-rated, how did their rating compare to external raters'?
self_perception_ideas <- compliant_self_perception %>% 
  anti_join(unevaluated_conversations, by=c("submitter_id", "object")) %>% 
  dplyr::select(submitter_id, 
         condition, 
         object, 
         ends_with("_selected"), 
         unique, 
         creativity, 
         originality, 
         usefulness) %>% 
  pivot_longer(cols=ends_with("_selected"),
               names_to = "conversation",
               values_to = "use") %>% 
  drop_na(use) %>% 
  filter(str_detect(conversation, object)) %>%
  rename(self_originality = "originality",
         self_creativity = "creativity",
         self_usefulness = "usefulness") %>% 
  mutate(object=if_else(object=="bubble", "bubblewrap", object))

self_other_perceptions <- left_join(x=self_perception_ideas, y=iriss_manual_ratings_metrics) %>% 
  drop_na(creativity, originality, usefulness) %>% 
  group_by(submitter_id, condition, object, use) %>% 
  summarize(self_creativity = mean(self_creativity), 
            self_originality = mean(self_originality), 
            self_usefulness = mean(self_usefulness), 
            creativity = mean(creativity), 
            originality = mean(originality), 
            usefulness = mean(usefulness))

# Correlate self perceived with other perceived creativity

cor.test(self_other_perceptions$self_creativity, self_other_perceptions$creativity)

cor.test(self_other_perceptions$self_originality, self_other_perceptions$originality)

cor.test(self_other_perceptions$self_usefulness, self_other_perceptions$usefulness)


```


```{r}
# Does the difference in self-perceived vs. other-perceived creativity vary by condition?

perception_diffs <- self_other_perceptions %>% 
  mutate(creativity_diff = self_creativity-creativity,
         originality_diff = self_originality-originality,
         usefulness_diff = self_usefulness-usefulness)


perception_diffs %>% 
  group_by(condition) %>% 
  get_summary_stats(c(creativity_diff, originality_diff, usefulness_diff), type ="mean_sd") %>% 
  ggplot(aes(x=condition,y=mean)) +
  geom_point() +
  facet_wrap(~variable)

perception_summary <- perception_diffs %>% 
  group_by(condition) %>% 
  get_summary_stats(c(creativity_diff, originality_diff, usefulness_diff), 
                    type = "mean_sd") %>% 
  mutate(sem = sd / sqrt(n))  # calculate SEM

perception_summary %>% 
  ggplot(aes(x = condition, y = mean)) +
  geom_point() +
  geom_errorbar(aes(ymin = mean - sem, ymax = mean + sem), width = 0.1) +
  facet_wrap(~variable) +
  ylab("Mean  SEM")

# Statistical analysis

perception_diff_participants <- perception_diffs %>% 
  group_by(submitter_id, condition) %>% 
  summarize(creativity_diff = mean(creativity_diff),
            originality_diff = mean(originality_diff),
            usefulness_diff = mean(usefulness_diff))

creativity_diff_anova <- aov(creativity_diff ~ condition,
  data = perception_diff_participants
)
summary(creativity_diff_anova)

originality_diff_anova <- aov(originality_diff ~ condition, data=perception_diff_participants)
summary(originality_diff_anova)

usefulness_diff_anova <- aov(usefulness_diff ~ condition, data=perception_diff_participants)
summary(usefulness_diff_anova)

# # Predict difference score from condition, submitter ID, and object
# lmer(creativity_diff ~ condition + (1|submitter_id) + (1|object), data=perception_diffs) 
```


### Timing
```{r}
duration <- iriss %>% 
  select(submitter_id, condition, contains("Duration")) %>% 
  mutate(across(c(3:6), as.numeric)) %>% 
  rename(duration_sec = "Duration (in seconds)") %>% 
  mutate(duration_min = duration_sec/60) %>% 
  relocate(duration_min, .after=duration_sec)

duration %>% 
  get_summary_stats(duration_min, type = "five_number")

duration %>%
  anti_join(excluded_participants) %>% 
  group_by(condition) %>% 
  get_summary_stats(duration_mein, type="five_number")


# Timing variables refer to the amount of time it took the participant to read their condition's instructions
timing <- iriss %>% 
  select(submitter_id, condition, contains("Page Submit")) %>% 
  mutate(across(c(3:5), as.numeric)) %>% # Make all timer columns numeric
  pivot_longer(cols=starts_with("timer"), names_to = "trial", values_to = "duration") %>% 
  anti_join(excluded_participants, by="submitter_id")
```

```{r}
# Perceptions of uniqueness in the evaluated, compliant sample
uniqueness <- iriss_self_perceptions %>% 
  right_join(evaluated_compliant_conversations, by=c("submitter_id", "object")) %>% 
  mutate(unique=as.numeric(unique))

unique_lm <- lm(unique~condition.x, data=uniqueness)
summary(unique_lm)

# Create difference scores of actual vs. perceived creativity?
```

```{r}
intent <- read_csv("data/iriss_intent_classification.csv") %>% 
  rename(submitter_id="participant_id") %>% 
  anti_join(excluded_participants, by="submitter_id")

intent %>% 
  group_by(condition) %>% 
  summarize(mean_doing = mean(doing),
            sd_doing = sd(doing))


```

### Direct Asks
```{r}
direct_asks <- read_csv("data/direct_asks.csv") %>% 
  rename(submitter_id = "ResponseId") %>% 
  mutate(Condition=case_when(Condition=="passenger"~"Low-Agency",
                             Condition=="pilot"~"High-Agency")) %>% 
  rename(condition="Condition")

# Does the proportion of direct asks correlate with person-level homogeneity?

participant_compliant_similarity <- compliant_similarities %>%
  filter(condition %in% c("Low-Agency", "High-Agency")) %>% 
  group_by(submitter_id, condition) %>% 
  summarize(mean_similarity = mean(similarity))

direct_ask_similarity <- full_join(direct_asks, participant_compliant_similarity, by=c("submitter_id", "condition")) %>% 
    mutate(direct_ask_binary = if_else(direct_ask_proportion==0, 0, 1)) # If no direct asks, they get coded as Guided Ideation

ggplot(direct_ask_similarity, aes(x=direct_ask_proportion,y=mean_similarity)) +
  geom_point() +
  stat_smooth()

cor.test(direct_ask_similarity$direct_ask_proportion, direct_ask_similarity$mean_similarity, method="spearman")

direct_ask_sim_model <- lm(mean_similarity ~ direct_ask_proportion*condition, data = direct_ask_similarity)
summary(direct_ask_sim_model)


ggplot(direct_ask_similarity, aes(x=direct_ask_binary, y=mean_similarity)) +
  geom_point() 

# Logistic regression predicting direct ask from condition
direct_ask.log <- glm(direct_ask_binary ~ condition, family = "binomial", data=direct_ask_similarity)
summary(direct_ask.log)
exp(coef(direct_ask.log))

# Does direct ask binary predict mean similarity?

direct_ask.lm <- lm(mean_similarity ~ direct_ask_binary*condition, data=direct_ask_similarity)
summary(direct_ask.lm)
```

## Mediation Analysis

```{r}
# Convert Mindset to numeric (0 = Low-Agency, 1 = High-Agency)
direct_ask_similarity$condition_numeric <- if_else(direct_ask_similarity$condition == "High-Agency", 1, 0)

# ============================================================
# METHOD 1: Using the mediation package (Baron & Kenny approach)
# ============================================================

# Step 1: Total effect (c path): X -> Y
model_total <- lm(mean_similarity ~ condition_numeric, data = direct_ask_similarity)
summary(model_total)

# Step 2: Effect on mediator (a path): X -> M
model_mediator <- lm(direct_ask_binary ~ condition_numeric, data = direct_ask_similarity)
summary(model_mediator)

# Step 3: Direct effect (b and c' paths): X + M -> Y
model_outcome <- lm(mean_similarity ~ condition_numeric + direct_ask_binary, data = direct_ask_similarity)
summary(model_outcome)

# Mediation analysis with bootstrapping (recommended)
set.seed(123)  # for reproducibility
mediation_results <- mediate(
  model_mediator,
  model_outcome,
  treat = "condition_numeric",
  mediator = "direct_ask_binary",
  boot = TRUE,
  sims = 5000
)

summary(mediation_results)

```
Direct Ask doesn't seem to mediate the relationship between mindset and mean within-person similarity. 

```{r}
biserial.cor(direct_ask_similarity$mean_similarity, direct_ask_similarity$direct_ask_binary)
cor.test(direct_ask_similarity$mean_similarity, direct_ask_similarity$direct_ask_binary)

weightedCorr(direct_ask_similarity$mean_similarity, direct_ask_similarity$direct_ask_binary, method="Pearson", weights=direct_ask_similarity$total_count)


```


## Planning for Main Study

```{r}
# Omnibus ANOVA
pwr.anova.test(k = 3, f = 0.25, sig.level = 0.05, power = 0.95)

# Pairwise comparisons
pwr.t.test(d = 0.5, sig.level = 0.017, power = 0.95, type = "two.sample")

```

